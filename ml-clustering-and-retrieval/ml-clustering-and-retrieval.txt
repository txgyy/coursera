{"slug":"ml-clustering-and-retrieval","name":"Machine Learning: Clustering & Retrieval","description":"Case Studies: Finding Similar Documents\n\nA reader is interested in a specific news article and you want to find similar articles to recommend.  What is the right notion of similarity?  Moreover, what if there are millions of other documents?  Each time you want to a retrieve a new document, do you need to search through all other documents?  How do you group similar documents together?  How do you discover new, emerging topics that the documents cover?   \n\nIn this third case study, finding similar documents, you will examine similarity-based algorithms for retrieval.  In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA).  You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Create a document retrieval system using k-nearest neighbors.\n   -Identify various similarity metrics for text data.\n   -Reduce computations in k-nearest neighbor search by using KD-trees.\n   -Produce approximate nearest neighbors using locality sensitive hashing.\n   -Compare and contrast supervised and unsupervised learning tasks.\n   -Cluster documents by topic using k-means.\n   -Describe how to parallelize k-means using MapReduce.\n   -Examine probabilistic clustering approaches using mixtures models.\n   -Fit a mixture of Gaussian model using expectation maximization (EM).\n   -Perform mixed membership modeling using latent Dirichlet allocation (LDA).\n   -Describe the steps of a Gibbs sampler and how to use its output to draw inferences.\n   -Compare and contrast initialization techniques for non-convex optimization objectives.\n   -Implement these techniques in Python.","promoPhoto":"https://coursera-course-photos.s3.amazonaws.com/80/907e004d0011e5aa3207874406679c/gears-818461_1280.jpg","primaryLanguageCodes":["en"],"subtitleLanguageCodes":[],"estimatedWorkload":"6 weeks of study, 5-8 hours/week","instructorIds":[14032411,14033965],"partnerIds":[15],"categoryIds":[1],"overridePartnerLogos":{},"plannedLaunchDate":"June 30, 2016","domainTypes":[{"subdomainId":"data-analysis","domainId":"data-science"},{"subdomainId":"machine-learning","domainId":"data-science"}],"faqs":[],"id":"-N44X0IJEeWpogr5ZO8qxQ","isReal":true,"launchedAt":1467306095160,"isVerificationEnabled":true,"enrollableSiteUserRoles":[],"verificationEnabledAt":1440699895561,"isSubtitleTranslationEnabled":true,"previewUserIds":[],"isRestrictedMembership":false,"premiumExperienceVariant":"PremiumGrading","preEnrollmentEnabledAt":1441044034814,"sessionsEnabledAt":1466462461216,"s3Prefix":"-N44X0IJEeWpogr5ZO8qxQ","courseMaterial":{"elements":[{"id":"lnYv6","name":"Welcome","description":"Clustering and retrieval are some of the most high-impact machine learning tools out there.  Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform.  Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.","elements":[{"id":"b7n4o","name":"What is this course about?","elements":[{"id":"0wM5v","name":"Important Update regarding the Machine Learning specialization","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"MAEHg-c3EeaATA4NLfV-eA@3"}},"slug":"important-update-regarding-the-machine-learning-specialization","timeCommitment":600000},{"id":"GMFGL","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"aNUuHTnUEeaVNBKX0rYX6w@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"gEob2","name":"Welcome and introduction to clustering and retrieval tasks","content":{"typeName":"lecture","definition":{"duration":384000,"videoId":"Nhwg6jmsEeaorBKVHazGxQ","inVideoAssessmentId":"X0QDIzyiEeamqw6M_DD0iw@6","assets":[]}},"slug":"welcome-and-introduction-to-clustering-and-retrieval-tasks","timeCommitment":384000},{"id":"sSANE","name":"Course overview","content":{"typeName":"lecture","definition":{"duration":209000,"videoId":"SIATvzmsEeanshK8cmmryQ","assets":[]}},"slug":"course-overview","timeCommitment":209000},{"id":"LTQf0","name":"Module-by-module topics covered","content":{"typeName":"lecture","definition":{"duration":531000,"videoId":"V2pv-zmsEeaigwoj5IULiQ","assets":[]}},"slug":"module-by-module-topics-covered","timeCommitment":531000},{"id":"Jw7RS","name":"Assumed background","content":{"typeName":"lecture","definition":{"duration":371000,"videoId":"FXz7CTnYEeaGIg583yb9Iw","assets":[]}},"slug":"assumed-background","timeCommitment":371000},{"id":"iF7Ji","name":"Software tools you'll need for this course","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"dlQpqT5YEea6Mg4NHdan6Q@3"}},"slug":"software-tools-you-ll-need-for-this-course","timeCommitment":600000},{"id":"NcHMF","name":"A big week ahead!","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"ijzoBkTcEeak_xLBOsWDZw@2"}},"slug":"a-big-week-ahead","timeCommitment":600000}],"slug":"what-is-this-course-about","timeCommitment":3895000}],"slug":"welcome","timeCommitment":3895000},{"id":"NWHgj","name":"Nearest Neighbor Search","description":"We start the course by considering a retrieval task of fetching a document similar to one someone is currently reading.  We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses.  However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints.  You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces.  You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced.","elements":[{"id":"ArseK","name":"Introduction to nearest neighbor search and algorithms","elements":[{"id":"E2p45","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"REwJxjnVEealWA47kCBuKw@4"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"DgiQQ","name":"Retrieval as k-nearest neighbor search","content":{"typeName":"lecture","definition":{"duration":169000,"videoId":"WAztETnYEeadZArWokJ2PQ","assets":[]}},"slug":"retrieval-as-k-nearest-neighbor-search","timeCommitment":169000},{"id":"6wIOD","name":"1-NN algorithm","content":{"typeName":"lecture","definition":{"duration":175000,"videoId":"dG8F7zmsEeaTQRLfJl9fdQ","assets":[]}},"slug":"1-nn-algorithm","timeCommitment":175000},{"id":"6Hsy4","name":"k-NN algorithm","content":{"typeName":"lecture","definition":{"duration":409000,"videoId":"iB1ZX0TbEeaPQg7xnvmStw","assets":[]}},"slug":"k-nn-algorithm","timeCommitment":409000}],"slug":"introduction-to-nearest-neighbor-search-and-algorithms","timeCommitment":1353000},{"id":"sclyr","name":"The importance of data representations and distance metrics","elements":[{"id":"nl267","name":"Document representation","content":{"typeName":"lecture","definition":{"duration":352000,"videoId":"iAnK3jmsEeaorBKVHazGxQ","assets":[]}},"slug":"document-representation","timeCommitment":352000},{"id":"yDJrZ","name":"Distance metrics: Euclidean and scaled Euclidean","content":{"typeName":"lecture","definition":{"duration":402000,"videoId":"xW-QmzmsEeaTQRLfJl9fdQ","assets":[]}},"slug":"distance-metrics-euclidean-and-scaled-euclidean","timeCommitment":402000},{"id":"JMU6y","name":"Writing (scaled) Euclidean distance using (weighted) inner products","content":{"typeName":"lecture","definition":{"duration":241000,"videoId":"z0EmCDmsEeaAVw5NI5pAoQ","assets":[]}},"slug":"writing-scaled-euclidean-distance-using-weighted-inner-products","timeCommitment":241000},{"id":"yyegc","name":"Distance metrics: Cosine similarity","content":{"typeName":"lecture","definition":{"duration":540000,"videoId":"2pq51jmsEeaKyQ4GBStLWQ","assets":[]}},"slug":"distance-metrics-cosine-similarity","timeCommitment":540000},{"id":"ozHef","name":"To normalize or not and other distance considerations","content":{"typeName":"lecture","definition":{"duration":419000,"videoId":"5FDXBjmsEeaigwoj5IULiQ","assets":[]}},"slug":"to-normalize-or-not-and-other-distance-considerations","timeCommitment":419000},{"id":"YV0W4","name":"Representations and metrics","content":{"typeName":"exam","definition":{"questionCount":6,"assessmentId":"5BiN3Dm_Eeaigwoj5IULiQ@16","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":6,"learningObjectives":[]}},"slug":"representations-and-metrics","timeCommitment":720000}],"slug":"the-importance-of-data-representations-and-distance-metrics","timeCommitment":2674000},{"id":"DBoZA","name":"Programming Assignment 1","elements":[{"id":"gJSfc","name":"Choosing features and metrics for nearest neighbor search","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"FO-xUDKUEeaeDxLc85lKyw@41"}},"slug":"choosing-features-and-metrics-for-nearest-neighbor-search","timeCommitment":600000},{"id":"nC8Mq","name":"Choosing features and metrics for nearest neighbor search","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"HGaMsDKUEeaXjBJ7D9QU2w@4","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"choosing-features-and-metrics-for-nearest-neighbor-search","timeCommitment":600000}],"slug":"programming-assignment-1","timeCommitment":1200000},{"id":"T4RKm","name":"Scaling up k-NN search using KD-trees","elements":[{"id":"5R6q3","name":"Complexity of brute force search","content":{"typeName":"lecture","definition":{"duration":110000,"videoId":"-gS4DTmsEeaQcgreU7dlZw","assets":[]}},"slug":"complexity-of-brute-force-search","timeCommitment":110000},{"id":"S0gfp","name":"KD-tree representation","content":{"typeName":"lecture","definition":{"duration":578000,"videoId":"CXkEBDmtEeadZArWokJ2PQ","assets":[]}},"slug":"kd-tree-representation","timeCommitment":578000},{"id":"6eTzw","name":"NN search with KD-trees","content":{"typeName":"lecture","definition":{"duration":428000,"videoId":"HCSHJDmtEeaRVAqopC17_Q","assets":[]}},"slug":"nn-search-with-kd-trees","timeCommitment":428000},{"id":"BkZTg","name":"Complexity of NN search with KD-trees","content":{"typeName":"lecture","definition":{"duration":339000,"videoId":"ewFKtjnYEea2BQ5PotcRIQ","assets":[]}},"slug":"complexity-of-nn-search-with-kd-trees","timeCommitment":339000},{"id":"2i4c7","name":"Visualizing scaling behavior of KD-trees","content":{"typeName":"lecture","definition":{"duration":263000,"videoId":"LSMc1TmtEeaycQrbEMp1ZQ","assets":[]}},"slug":"visualizing-scaling-behavior-of-kd-trees","timeCommitment":263000},{"id":"xwFl8","name":"Approximate k-NN search using KD-trees","content":{"typeName":"lecture","definition":{"duration":465000,"videoId":"NRjspzmtEealWA47kCBuKw","assets":[]}},"slug":"approximate-k-nn-search-using-kd-trees","timeCommitment":465000},{"id":"uJh72","name":"(OPTIONAL) A worked-out example for KD-trees","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"-HYRf3PeEea1xgrjVVNMkQ@5"}},"slug":"optional-a-worked-out-example-for-kd-trees","timeCommitment":600000},{"id":"0acPr","name":"KD-trees","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"i1ivaTnOEea6Vgps67kifw@10","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"kd-trees","timeCommitment":600000}],"slug":"scaling-up-k-nn-search-using-kd-trees","timeCommitment":3383000},{"id":"oKxBC","name":"Locality sensitive hashing for approximate NN search","elements":[{"id":"DA2Hg","name":"Limitations of KD-trees","content":{"typeName":"lecture","definition":{"duration":213000,"videoId":"R58MkzmtEealbwr82FuyJw","assets":[]}},"slug":"limitations-of-kd-trees","timeCommitment":213000},{"id":"HfFv9","name":"LSH as an alternative to KD-trees","content":{"typeName":"lecture","definition":{"duration":260000,"videoId":"WV0iNzmtEeaE6BKpFFMXTw","assets":[]}},"slug":"lsh-as-an-alternative-to-kd-trees","timeCommitment":260000},{"id":"KFXqi","name":"Using random lines to partition points","content":{"typeName":"lecture","definition":{"duration":340000,"videoId":"aNRUljmtEeanshK8cmmryQ","assets":[]}},"slug":"using-random-lines-to-partition-points","timeCommitment":340000},{"id":"BZZ0P","name":"Defining more bins","content":{"typeName":"lecture","definition":{"duration":208000,"videoId":"dE0J8TmtEea2BQ5PotcRIQ","assets":[]}},"slug":"defining-more-bins","timeCommitment":208000},{"id":"WEiAC","name":"Searching neighboring bins","content":{"typeName":"lecture","definition":{"duration":517000,"videoId":"kw32TjmtEeaB7xKuVDiLMw","assets":[]}},"slug":"searching-neighboring-bins","timeCommitment":517000},{"id":"Rjk5a","name":"LSH in higher dimensions","content":{"typeName":"lecture","definition":{"duration":249000,"videoId":"nOwOMTmtEeaycQrbEMp1ZQ","assets":[]}},"slug":"lsh-in-higher-dimensions","timeCommitment":249000},{"id":"aZCHX","name":"(OPTIONAL) Improving efficiency through multiple tables","content":{"typeName":"lecture","definition":{"duration":1366000,"videoId":"ra-XLTmtEeaQcgreU7dlZw","assets":[]}},"slug":"optional-improving-efficiency-through-multiple-tables","timeCommitment":1366000},{"id":"7cVMj","name":"Locality Sensitive Hashing","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"cGCfTjnQEea2BQ5PotcRIQ@6","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"locality-sensitive-hashing","timeCommitment":600000}],"slug":"locality-sensitive-hashing-for-approximate-nn-search","timeCommitment":3753000},{"id":"BQFwA","name":"Programming Assignment 2","elements":[{"id":"mMipk","name":"Implementing Locality Sensitive Hashing from scratch","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"N5gUTjg-EeanRxKqUCErxw@28"}},"slug":"implementing-locality-sensitive-hashing-from-scratch","timeCommitment":600000},{"id":"itzL5","name":"Implementing Locality Sensitive Hashing from scratch","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"8en5Izg6EeacVQ41sefswQ@3","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"implementing-locality-sensitive-hashing-from-scratch","timeCommitment":600000}],"slug":"programming-assignment-2","timeCommitment":1200000},{"id":"bADP9","name":"Summarizing nearest neighbor search","elements":[{"id":"d7mz7","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":149000,"videoId":"x6AQGTmtEeaycQrbEMp1ZQ","assets":[]}},"slug":"a-brief-recap","timeCommitment":149000}],"slug":"summarizing-nearest-neighbor-search","timeCommitment":149000}],"slug":"nearest-neighbor-search","timeCommitment":13712000},{"id":"qlpMT","name":"Clustering with k-means","description":"In clustering, our goal is to group the datapoints in our dataset into disjoint sets.  Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by \"topic\".  These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like \"Science\", \"World News\", etc.  Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset.  The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there.  To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework.  You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned.","elements":[{"id":"fyXpr","name":"Introduction to clustering","elements":[{"id":"ZuPHd","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"eOrNsznVEealWA47kCBuKw@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"yXJLN","name":"The goal of clustering","content":{"typeName":"lecture","definition":{"duration":206000,"videoId":"slFgWTm1Eea2BQ5PotcRIQ","assets":[]}},"slug":"the-goal-of-clustering","timeCommitment":206000},{"id":"vRE7c","name":"An unsupervised task","content":{"typeName":"lecture","definition":{"duration":402000,"videoId":"wtYx5Dm1Eea--w7I7gV3Uw","assets":[]}},"slug":"an-unsupervised-task","timeCommitment":402000},{"id":"DIwqE","name":"Hope for unsupervised learning, and some challenge cases","content":{"typeName":"lecture","definition":{"duration":248000,"videoId":"zLMRWzm1Eea2BQ5PotcRIQ","assets":[]}},"slug":"hope-for-unsupervised-learning-and-some-challenge-cases","timeCommitment":248000}],"slug":"introduction-to-clustering","timeCommitment":1456000},{"id":"PagkH","name":"Clustering via k-means","elements":[{"id":"JZ62p","name":"The k-means algorithm","content":{"typeName":"lecture","definition":{"duration":466000,"videoId":"1ycniTm1EeaycQrbEMp1ZQ","assets":[]}},"slug":"the-k-means-algorithm","timeCommitment":466000},{"id":"Fb58J","name":"k-means as coordinate descent","content":{"typeName":"lecture","definition":{"duration":361000,"videoId":"7BT5ATm1Eea6Vgps67kifw","assets":[]}},"slug":"k-means-as-coordinate-descent","timeCommitment":361000},{"id":"T9ZaG","name":"Smart initialization via k-means++","content":{"typeName":"lecture","definition":{"duration":288000,"videoId":"-UlA_Tm1Eealbwr82FuyJw","assets":[]}},"slug":"smart-initialization-via-k-means","timeCommitment":288000},{"id":"JVWne","name":"Assessing the quality and choosing the number of clusters","content":{"typeName":"lecture","definition":{"duration":567000,"videoId":"AyenEDm2EeaK-BLbPXu8HQ","assets":[]}},"slug":"assessing-the-quality-and-choosing-the-number-of-clusters","timeCommitment":567000},{"id":"tHtXY","name":"k-means","content":{"typeName":"exam","definition":{"questionCount":9,"assessmentId":"6krzyTnQEeaRVAqopC17_Q@3","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":12,"learningObjectives":[]}},"slug":"k-means","timeCommitment":1080000}],"slug":"clustering-via-k-means","timeCommitment":2762000},{"id":"ymzlj","name":"Programming Assignment","elements":[{"id":"E26k9","name":"Clustering text data with k-means","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"jxBwtDNIEeaFThIQfrwuFQ@30"}},"slug":"clustering-text-data-with-k-means","timeCommitment":600000},{"id":"2yRba","name":"Clustering text data with K-means","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"hfZkpzNWEeaSGQo91ycJfQ@7","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":8,"learningObjectives":[]}},"slug":"clustering-text-data-with-k-means","timeCommitment":960000}],"slug":"programming-assignment","timeCommitment":1560000},{"id":"mycgE","name":"MapReduce for scaling k-means","elements":[{"id":"Jz3Ex","name":"Motivating MapReduce","content":{"typeName":"lecture","definition":{"duration":527000,"videoId":"ZCJvOzneEeaB7xKuVDiLMw","assets":[]}},"slug":"motivating-mapreduce","timeCommitment":527000},{"id":"5GWtU","name":"The general MapReduce abstraction","content":{"typeName":"lecture","definition":{"duration":315000,"videoId":"GDXRTTm2Eea2BQ5PotcRIQ","assets":[]}},"slug":"the-general-mapreduce-abstraction","timeCommitment":315000},{"id":"rYLP1","name":"MapReduce execution overview and combiners","content":{"typeName":"lecture","definition":{"duration":380000,"videoId":"LbsoCDm2EeaGIg583yb9Iw","assets":[]}},"slug":"mapreduce-execution-overview-and-combiners","timeCommitment":380000},{"id":"EhCYk","name":"MapReduce for k-means","content":{"typeName":"lecture","definition":{"duration":441000,"videoId":"OkF8Lzm2EeaaLwqSFQWmLw","assets":[]}},"slug":"mapreduce-for-k-means","timeCommitment":441000},{"id":"dlBVz","name":"MapReduce for k-means","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"fLfi7jnSEeadZArWokJ2PQ@7","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"mapreduce-for-k-means","timeCommitment":600000}],"slug":"mapreduce-for-scaling-k-means","timeCommitment":2263000},{"id":"kOoeA","name":"Summarizing clustering with k-means","elements":[{"id":"ADVKF","name":"Other applications of clustering","content":{"typeName":"lecture","definition":{"duration":435000,"videoId":"RTzEAjm2EeaRVAqopC17_Q","assets":[]}},"slug":"other-applications-of-clustering","timeCommitment":435000},{"id":"ypfzW","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":88000,"videoId":"xm9MhTm3EeaB7xKuVDiLMw","assets":[]}},"slug":"a-brief-recap","timeCommitment":88000}],"slug":"summarizing-clustering-with-k-means","timeCommitment":523000}],"slug":"clustering-with-k-means","timeCommitment":8564000},{"id":"k2MXQ","name":"Mixture Models","description":"In k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information.  In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a \"cluster\" and (2) accounts for uncertainty in assignments of datapoints to clusters via \"soft assignments\".  You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters.  To gain intuition, you will first consider a visually appealing image clustering task.  You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered.","elements":[{"id":"8jUzO","name":"Motivating and setting the foundation for mixture models","elements":[{"id":"bQ5bW","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"nlOGmTnVEea6Vgps67kifw@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"I6FYH","name":"Motiving probabilistic clustering models","content":{"typeName":"lecture","definition":{"duration":481000,"videoId":"XoswCTm2EeaK-BLbPXu8HQ","assets":[]}},"slug":"motiving-probabilistic-clustering-models","timeCommitment":481000},{"id":"olZKI","name":"Aggregating over unknown classes in an image dataset","content":{"typeName":"lecture","definition":{"duration":400000,"videoId":"bcnHADm2Eea--w7I7gV3Uw","assets":[]}},"slug":"aggregating-over-unknown-classes-in-an-image-dataset","timeCommitment":400000},{"id":"MnRLo","name":"Univariate Gaussian distributions","content":{"typeName":"lecture","definition":{"duration":168000,"videoId":"eSJJJTm2EeaRVAqopC17_Q","assets":[]}},"slug":"univariate-gaussian-distributions","timeCommitment":168000},{"id":"4IPSf","name":"Bivariate and multivariate Gaussians","content":{"typeName":"lecture","definition":{"duration":449000,"videoId":"KvLfwj3fEeakjhLAqpJdTQ","assets":[]}},"slug":"bivariate-and-multivariate-gaussians","timeCommitment":449000}],"slug":"motivating-and-setting-the-foundation-for-mixture-models","timeCommitment":2098000},{"id":"cHjpO","name":"Mixtures of Gaussians for clustering","elements":[{"id":"1BBlC","name":"Mixture of Gaussians","content":{"typeName":"lecture","definition":{"duration":394000,"videoId":"jDVAljm2EeaK-BLbPXu8HQ","assets":[]}},"slug":"mixture-of-gaussians","timeCommitment":394000},{"id":"fj1qT","name":"Interpreting the mixture of Gaussian terms","content":{"typeName":"lecture","definition":{"duration":346000,"videoId":"lbyFyzm2Eealbwr82FuyJw","assets":[]}},"slug":"interpreting-the-mixture-of-gaussian-terms","timeCommitment":346000},{"id":"tFalw","name":"Scaling mixtures of Gaussians for document clustering","content":{"typeName":"lecture","definition":{"duration":313000,"videoId":"njZ5Zjm2EeaTQRLfJl9fdQ","assets":[]}},"slug":"scaling-mixtures-of-gaussians-for-document-clustering","timeCommitment":313000}],"slug":"mixtures-of-gaussians-for-clustering","timeCommitment":1053000},{"id":"ixCal","name":"Expectation Maximization (EM) building blocks","elements":[{"id":"lccM4","name":"Computing soft assignments from known cluster parameters","content":{"typeName":"lecture","definition":{"duration":449000,"videoId":"qE93mTm2EeaGIg583yb9Iw","assets":[]}},"slug":"computing-soft-assignments-from-known-cluster-parameters","timeCommitment":449000},{"id":"HXWEK","name":"(OPTIONAL) Responsibilities as Bayes' rule","content":{"typeName":"lecture","definition":{"duration":313000,"videoId":"s6M-9zm2Eealbwr82FuyJw","assets":[]}},"slug":"optional-responsibilities-as-bayes-rule","timeCommitment":313000},{"id":"flx8n","name":"Estimating cluster parameters from known cluster assignments","content":{"typeName":"lecture","definition":{"duration":414000,"videoId":"jbrN6T3fEea2sxLHPsXumw","assets":[]}},"slug":"estimating-cluster-parameters-from-known-cluster-assignments","timeCommitment":414000},{"id":"rBAK1","name":"Estimating cluster parameters from soft assignments","content":{"typeName":"lecture","definition":{"duration":481000,"videoId":"w_n8Dzm2Eea9vQqeg-LyDQ","assets":[]}},"slug":"estimating-cluster-parameters-from-soft-assignments","timeCommitment":481000}],"slug":"expectation-maximization-em-building-blocks","timeCommitment":1657000},{"id":"qfG12","name":"The EM algorithm","elements":[{"id":"byVa8","name":"EM iterates in equations and pictures","content":{"typeName":"lecture","definition":{"duration":399000,"videoId":"y9leCTm2EeanshK8cmmryQ","assets":[]}},"slug":"em-iterates-in-equations-and-pictures","timeCommitment":399000},{"id":"opLcN","name":"Convergence, initialization, and overfitting of EM","content":{"typeName":"lecture","definition":{"duration":562000,"videoId":"0-F9DDm2EeaLDg72c98SCQ","assets":[]}},"slug":"convergence-initialization-and-overfitting-of-em","timeCommitment":562000},{"id":"gXzK4","name":"Relationship to k-means","content":{"typeName":"lecture","definition":{"duration":191000,"videoId":"3m06ETm2EeaLDg72c98SCQ","assets":[]}},"slug":"relationship-to-k-means","timeCommitment":191000},{"id":"s9OBQ","name":"(OPTIONAL) A worked-out example for EM","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"UlGIgGtmEeahqg7RMlfxsw@4"}},"slug":"optional-a-worked-out-example-for-em","timeCommitment":600000},{"id":"QVHj1","name":"EM for Gaussian mixtures","content":{"typeName":"exam","definition":{"questionCount":9,"assessmentId":"_NankznSEea2BQ5PotcRIQ@11","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":10,"learningObjectives":[]}},"slug":"em-for-gaussian-mixtures","timeCommitment":1080000}],"slug":"the-em-algorithm","timeCommitment":2832000},{"id":"72eme","name":"Summarizing mixture models","elements":[{"id":"aAfaO","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":100000,"videoId":"5mcBdDm2Eea2BQ5PotcRIQ","assets":[]}},"slug":"a-brief-recap","timeCommitment":100000}],"slug":"summarizing-mixture-models","timeCommitment":100000},{"id":"nrL4y","name":"Programming Assignment 1","elements":[{"id":"XbP9U","name":"Implementing EM for Gaussian mixtures","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"2sBUAjNLEear8wp6NX1ctQ@29"}},"slug":"implementing-em-for-gaussian-mixtures","timeCommitment":600000},{"id":"UnnXU","name":"Implementing EM for Gaussian mixtures","content":{"typeName":"exam","definition":{"questionCount":6,"assessmentId":"TaVNGzM5EeaR4wo8_PkQSQ@13","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":6,"learningObjectives":[]}},"slug":"implementing-em-for-gaussian-mixtures","timeCommitment":720000}],"slug":"programming-assignment-1","timeCommitment":1320000},{"id":"th38c","name":"Programming Assignment 2","elements":[{"id":"XGgeJ","name":"Clustering text data with Gaussian mixtures","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"c6sDMTNWEeaiUw7KVUxIhQ@26"}},"slug":"clustering-text-data-with-gaussian-mixtures","timeCommitment":600000},{"id":"unHNR","name":"Clustering text data with Gaussian mixtures","content":{"typeName":"exam","definition":{"questionCount":4,"assessmentId":"3MY1_jNLEeav6w6zGMT5wQ@7","gradingWeight":8,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":4,"learningObjectives":[]}},"slug":"clustering-text-data-with-gaussian-mixtures","timeCommitment":480000}],"slug":"programming-assignment-2","timeCommitment":1080000}],"slug":"mixture-models","timeCommitment":10140000},{"id":"S7Yd4","name":"Mixed Membership Modeling via Latent Dirichlet Allocation","description":"The clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic.  But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics.  In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis.  You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features.  The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations.<p>Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling.  You will be able to implement a Gibbs sampler for LDA by the end of the module.","elements":[{"id":"5swcI","name":"Introduction to latent Dirichlet allocation","elements":[{"id":"RifjP","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"x56pJznVEeaRVAqopC17_Q@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"hQBJI","name":"Mixed membership models for documents","content":{"typeName":"lecture","definition":{"duration":214000,"videoId":"LdbRbTm3EeaRVAqopC17_Q","assets":[]}},"slug":"mixed-membership-models-for-documents","timeCommitment":214000},{"id":"L9uzZ","name":"An alternative document clustering model","content":{"typeName":"lecture","definition":{"duration":275000,"videoId":"7OjyZjnXEeaB7xKuVDiLMw","assets":[]}},"slug":"an-alternative-document-clustering-model","timeCommitment":275000},{"id":"pfnoB","name":"Components of latent Dirichlet allocation model","content":{"typeName":"lecture","definition":{"duration":156000,"videoId":"-kksUznXEeaB7xKuVDiLMw","assets":[]}},"slug":"components-of-latent-dirichlet-allocation-model","timeCommitment":156000},{"id":"XySk4","name":"Goal of LDA inference","content":{"typeName":"lecture","definition":{"duration":302000,"videoId":"N8XYtjm3EeafEhKUwZ_Ayw","assets":[]}},"slug":"goal-of-lda-inference","timeCommitment":302000},{"id":"d6s1W","name":"Latent Dirichlet Allocation","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"gFyg2jnVEeaUpBJG2l0r1w@2","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"latent-dirichlet-allocation","timeCommitment":600000}],"slug":"introduction-to-latent-dirichlet-allocation","timeCommitment":2147000},{"id":"3VzlU","name":"Bayesian inference via Gibbs sampling","elements":[{"id":"dyjXr","name":"The need for Bayesian inference","content":{"typeName":"lecture","definition":{"duration":297000,"videoId":"Q1zqhzm3Eea2BQ5PotcRIQ","assets":[]}},"slug":"the-need-for-bayesian-inference","timeCommitment":297000},{"id":"s60DQ","name":"Gibbs sampling from 10,000 feet","content":{"typeName":"lecture","definition":{"duration":328000,"videoId":"TBPnwDm3EeaaLwqSFQWmLw","assets":[]}},"slug":"gibbs-sampling-from-10-000-feet","timeCommitment":328000},{"id":"T36G9","name":"A standard Gibbs sampler for LDA","content":{"typeName":"lecture","definition":{"duration":585000,"videoId":"U2JUAzm3EeaaLwqSFQWmLw","assets":[]}},"slug":"a-standard-gibbs-sampler-for-lda","timeCommitment":585000}],"slug":"bayesian-inference-via-gibbs-sampling","timeCommitment":1210000},{"id":"KQCMd","name":"Collapsed Gibbs sampling for LDA","elements":[{"id":"qC5gv","name":"What is collapsed Gibbs sampling?","content":{"typeName":"lecture","definition":{"duration":207000,"videoId":"WsvpJzm3Eea6Vgps67kifw","assets":[]}},"slug":"what-is-collapsed-gibbs-sampling","timeCommitment":207000},{"id":"XMwnP","name":"A worked example for LDA: Initial setup","content":{"typeName":"lecture","definition":{"duration":265000,"videoId":"Y_FfDDm3EeaUpBJG2l0r1w","assets":[]}},"slug":"a-worked-example-for-lda-initial-setup","timeCommitment":265000},{"id":"O35EG","name":"A worked example for LDA: Deriving the resampling distribution","content":{"typeName":"lecture","definition":{"duration":469000,"videoId":"deTcDjm3EeanshK8cmmryQ","assets":[]}},"slug":"a-worked-example-for-lda-deriving-the-resampling-distribution","timeCommitment":469000},{"id":"m4gR9","name":"Using the output of collapsed Gibbs sampling","content":{"typeName":"lecture","definition":{"duration":253000,"videoId":"gAoPrDm3EeaytA6niB5r5Q","assets":[]}},"slug":"using-the-output-of-collapsed-gibbs-sampling","timeCommitment":253000}],"slug":"collapsed-gibbs-sampling-for-lda","timeCommitment":1194000},{"id":"510No","name":"Summarizing latent Dirichlet allocation","elements":[{"id":"cV5XM","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":103000,"videoId":"qAhAITm3EeaQcgreU7dlZw","assets":[]}},"slug":"a-brief-recap","timeCommitment":103000},{"id":"6ieZu","name":"Learning LDA model via Gibbs sampling","content":{"typeName":"exam","definition":{"questionCount":10,"assessmentId":"2XKQVjnVEealbwr82FuyJw@6","gradingWeight":5,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":12,"learningObjectives":[]}},"slug":"learning-lda-model-via-gibbs-sampling","timeCommitment":1200000}],"slug":"summarizing-latent-dirichlet-allocation","timeCommitment":1303000},{"id":"BQ9Kw","name":"Programming Assignment","elements":[{"id":"OL4J1","name":"Modeling text topics with Latent Dirichlet Allocation","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"ooHYijNpEeaSGQo91ycJfQ@19"}},"slug":"modeling-text-topics-with-latent-dirichlet-allocation","timeCommitment":600000},{"id":"LXSnc","name":"Modeling text topics with Latent Dirichlet Allocation","content":{"typeName":"exam","definition":{"questionCount":12,"assessmentId":"mwM1YzNpEea6_g7q-N8lOQ@12","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":12,"learningObjectives":[]}},"slug":"modeling-text-topics-with-latent-dirichlet-allocation","timeCommitment":1440000}],"slug":"programming-assignment","timeCommitment":2040000}],"slug":"mixed-membership-modeling-via-latent-dirichlet-allocation","timeCommitment":7894000},{"id":"TSWbk","name":"Hierarchical Clustering & Closing Remarks","description":"In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful.<p>We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset.  Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series.  We then briefly outline some important clustering and retrieval ideas that we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  ","elements":[{"id":"gHUXr","name":"What we've learned","elements":[{"id":"Y6ytu","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"SnhrzER_EeaO2w5mUuk78Q@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"Z6Rrw","name":"Module 1 recap","content":{"typeName":"lecture","definition":{"duration":624000,"videoId":"6u4I2j1dEeaYigo3mhjrDw","assets":[]}},"slug":"module-1-recap","timeCommitment":624000},{"id":"azWZt","name":"Module 2 recap","content":{"typeName":"lecture","definition":{"duration":198000,"videoId":"Bk9PwD1eEeajAxJHgFkBXQ","assets":[]}},"slug":"module-2-recap","timeCommitment":198000},{"id":"OdLFM","name":"Module 3 recap","content":{"typeName":"lecture","definition":{"duration":371000,"videoId":"HnqZvj1eEeaqKBK0qGd0nw","assets":[]}},"slug":"module-3-recap","timeCommitment":371000},{"id":"cUjkK","name":"Module 4 recap","content":{"typeName":"lecture","definition":{"duration":437000,"videoId":"REijEz1eEeaFQw5EmPqRKw","assets":[]}},"slug":"module-4-recap","timeCommitment":437000}],"slug":"what-we-ve-learned","timeCommitment":2230000},{"id":"8KmNR","name":"Hierarchical clustering and clustering for time series segmentation","elements":[{"id":"P4kkl","name":"Why hierarchical clustering?","content":{"typeName":"lecture","definition":{"duration":142000,"videoId":"S_ktTUTWEeaogQqbuwBecQ","assets":[]}},"slug":"why-hierarchical-clustering","timeCommitment":142000},{"id":"gp6sI","name":"Divisive clustering","content":{"typeName":"lecture","definition":{"duration":249000,"videoId":"joPvmkTWEeaKQgrFZY7n_Q","assets":[]}},"slug":"divisive-clustering","timeCommitment":249000},{"id":"bsFBT","name":"Agglomerative clustering","content":{"typeName":"lecture","definition":{"duration":165000,"videoId":"qZHl7ETWEeaM4A6TpIj4EQ","assets":[]}},"slug":"agglomerative-clustering","timeCommitment":165000},{"id":"MfcBU","name":"The dendrogram","content":{"typeName":"lecture","definition":{"duration":296000,"videoId":"7C97rkTbEeaKQgrFZY7n_Q","assets":[]}},"slug":"the-dendrogram","timeCommitment":296000},{"id":"mFZqI","name":"Agglomerative clustering details","content":{"typeName":"lecture","definition":{"duration":423000,"videoId":"44gol0TWEeav0RKKZ8W7hw","assets":[]}},"slug":"agglomerative-clustering-details","timeCommitment":423000},{"id":"Au5lm","name":"Hidden Markov models","content":{"typeName":"lecture","definition":{"duration":564000,"videoId":"CTRSZkTXEeaNXArC7gXnTQ","assets":[]}},"slug":"hidden-markov-models","timeCommitment":564000}],"slug":"hierarchical-clustering-and-clustering-for-time-series-segmentation","timeCommitment":1839000},{"id":"1t7xo","name":"Programming Assignment","elements":[{"id":"57vnY","name":"Modeling text data with a hierarchy of clusters","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"AcMvHT13EeauSQ69_Utoiw@16"}},"slug":"modeling-text-data-with-a-hierarchy-of-clusters","timeCommitment":600000},{"id":"QBFNf","name":"Modeling text data with a hierarchy of clusters","content":{"typeName":"exam","definition":{"questionCount":3,"assessmentId":"uTYzIz1vEeal8BKKq7kaIQ@8","gradingWeight":7,"passingFraction":0.33,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":3,"learningObjectives":[]}},"slug":"modeling-text-data-with-a-hierarchy-of-clusters","timeCommitment":360000}],"slug":"programming-assignment","timeCommitment":960000},{"id":"SXI5X","name":"Summary and what's ahead in the specialization","elements":[{"id":"3IOFe","name":"What we didn't cover","content":{"typeName":"lecture","definition":{"duration":161000,"videoId":"Wcv-Sz1eEeaFQw5EmPqRKw","assets":[]}},"slug":"what-we-didn-t-cover","timeCommitment":161000},{"id":"Vj4s0","name":"Thank you!","content":{"typeName":"lecture","definition":{"duration":97000,"videoId":"hQU1UD1eEeaqKBK0qGd0nw","assets":[]}},"slug":"thank-you","timeCommitment":97000}],"slug":"summary-and-whats-ahead-in-the-specialization","timeCommitment":258000}],"slug":"hierarchical-clustering-closing-remarks","timeCommitment":5287000}]}}