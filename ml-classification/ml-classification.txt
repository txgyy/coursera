{"slug":"ml-classification","name":"Machine Learning: Classification","description":"Case Studies: Analyzing Sentiment & Loan Default Prediction\n\nIn our case study on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...).  In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. \n\nIn this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks.  You will become familiar with  the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting.  In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent.  You will implement these technique on real-world, large-scale machine learning tasks.  You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier.  This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data.  We've also included optional content in every module, covering advanced topics for those who want to go even deeper! \n\nLearning Objectives: By the end of this course, you will be able to:\n   -Describe the input and output of a classification model.\n   -Tackle both binary and multiclass classification problems.\n   -Implement a logistic regression model for large-scale classification.  \n   -Create a non-linear model using decision trees.\n   -Improve the performance of any model using boosting.\n   -Scale your methods with stochastic gradient ascent.\n   -Describe the underlying decision boundaries.  \n   -Build a classification model to predict sentiment in a product review dataset.  \n   -Analyze financial data to predict loan defaults.\n   -Use techniques for handling missing data.\n   -Evaluate your models using precision-recall metrics.\n   -Implement these techniques in Python (or in the language of your choice, though Python is highly recommended).","promoPhoto":"https://coursera-course-photos.s3.amazonaws.com/b1/37a1a04d0011e59bd777c2a1ef6a6e/gears-818461_1280.jpg","primaryLanguageCodes":["en"],"subtitleLanguageCodes":[],"estimatedWorkload":"7 weeks of study, 5-8 hours/week","instructorIds":[14033965,14032411],"partnerIds":[15],"categoryIds":[1],"overridePartnerLogos":{},"plannedLaunchDate":"March 4, 2016","domainTypes":[{"subdomainId":"machine-learning","domainId":"data-science"}],"faqs":[],"id":"3c1bSkIJEeWpogr5ZO8qxQ","isReal":true,"launchedAt":1457134575927,"isVerificationEnabled":true,"enrollableSiteUserRoles":[],"verificationEnabledAt":1440699895561,"isSubtitleTranslationEnabled":true,"previewUserIds":[],"isRestrictedMembership":false,"preEnrollmentEnabledAt":1441043992547,"sessionsEnabledAt":1457134196441,"s3Prefix":"3c1bSkIJEeWpogr5ZO8qxQ","courseMaterial":{"elements":[{"id":"yu8ai","name":"Welcome!","description":"Classification is one of the most widely used techniques in machine learning, with a broad array of applications, including sentiment analysis, ad targeting, spam detection, risk assessment, medical diagnosis and image classification. The core goal of classification is to predict a category or class y from some inputs x. Through this course, you will become familiar with the fundamental models and algorithms used in classification, as well as a number of core machine learning concepts. Rather than covering all aspects of classification, you will focus on a few core techniques, which are widely used in the real-world to get state-of-the-art performance. By following our hands-on approach, you will implement your own algorithms on multiple real-world tasks, and deeply grasp the core techniques needed to be successful with these approaches in practice. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.","elements":[{"id":"c81Lw","name":"Welcome to the course","elements":[{"id":"SuXzp","name":"Important Update regarding the Machine Learning Specialization","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"GhAdpOc3EeaFSw4mnuhT5g@3"}},"slug":"important-update-regarding-the-machine-learning-specialization","timeCommitment":600000},{"id":"JbXKf","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"1CLSL9twEeWirA70Ynl3tQ@6"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"YMpzf","name":"Welcome to the classification course, a part of the Machine Learning Specialization","content":{"typeName":"lecture","definition":{"duration":82000,"videoId":"uveatdphEeWPHw6r45-nxw","assets":[]}},"slug":"welcome-to-the-classification-course-a-part-of-the-machine-learning","timeCommitment":82000},{"id":"qZhKx","name":"What is this course about?","content":{"typeName":"lecture","definition":{"duration":367000,"videoId":"xr69e9phEeWgOApRIOF5pQ","assets":[]}},"slug":"what-is-this-course-about","timeCommitment":367000},{"id":"OnpWH","name":"Impact of classification","content":{"typeName":"lecture","definition":{"duration":64000,"videoId":"0zmgbdphEeWXuxJgUJEB-Q","assets":[]}},"slug":"impact-of-classification","timeCommitment":64000}],"slug":"welcome-to-the-course","timeCommitment":1713000},{"id":"60pde","name":"Course overview and details","elements":[{"id":"84fuF","name":"Course overview","content":{"typeName":"lecture","definition":{"duration":196000,"videoId":"7kryPNphEeWJwRKcpT8ChQ","assets":[]}},"slug":"course-overview","timeCommitment":196000},{"id":"LyubT","name":"Outline of first half of course","content":{"typeName":"lecture","definition":{"duration":335000,"videoId":"-v37wtphEeWPHw6r45-nxw","assets":[]}},"slug":"outline-of-first-half-of-course","timeCommitment":335000},{"id":"z1g9k","name":"Outline of second half of course","content":{"typeName":"lecture","definition":{"duration":346000,"videoId":"CtN7k9piEeWPHw6r45-nxw","assets":[]}},"slug":"outline-of-second-half-of-course","timeCommitment":346000},{"id":"IindM","name":"Assumed background","content":{"typeName":"lecture","definition":{"duration":205000,"videoId":"HO4In9piEeWgOApRIOF5pQ","assets":[]}},"slug":"assumed-background","timeCommitment":205000},{"id":"AktDn","name":"Let's get started!","content":{"typeName":"lecture","definition":{"duration":45000,"videoId":"vIaaZtqNEeWUtQpvX4iAkw","assets":[]}},"slug":"lets-get-started","timeCommitment":45000},{"id":"JisIQ","name":"Reading: Software tools you'll need","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"3CN0M-ESEeW4CxI187KLiQ@8"}},"slug":"reading-software-tools-you-ll-need","timeCommitment":600000}],"slug":"course-overview-and-details","timeCommitment":1727000}],"slug":"welcome","timeCommitment":3440000},{"id":"0Wclc","name":"Linear Classifiers & Logistic Regression","description":"Linear classifiers are amongst the most practical classification methods. For example, in our sentiment analysis case-study, a linear classifier associates a coefficient with the counts of each word in the sentence. In this module, you will become proficient in this type of representation. You will focus on a particularly useful type of linear classifier called logistic regression, which, in addition to allowing you to predict a class, provides a probability associated with the prediction. These probabilities are extremely useful, since they provide a degree of confidence in the predictions. In this module, you will also be able to construct features from categorical inputs, and to tackle classification problems with more than two class (multiclass problems). You will examine the results of these techniques on a real-world product sentiment analysis task.","elements":[{"id":"6Mz8b","name":"Linear classifiers","elements":[{"id":"p3LZs","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"2IkK0dtzEeWDfA5_5YqObw@4"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"HNKIj","name":"Linear classifiers: A motivating example","content":{"typeName":"lecture","definition":{"duration":146000,"videoId":"SxyqjNaREeWCpQ7rT5LraQ","assets":[]}},"slug":"linear-classifiers-a-motivating-example","timeCommitment":146000},{"id":"lCBwS","name":"Intuition behind linear classifiers","content":{"typeName":"lecture","definition":{"duration":219000,"videoId":"n3JUiNaREeWBKwo1vzQ7vQ","assets":[]}},"slug":"intuition-behind-linear-classifiers","timeCommitment":219000},{"id":"NIdE0","name":"Decision boundaries","content":{"typeName":"lecture","definition":{"duration":197000,"videoId":"w2P9AtaREeWGkg42feWbDw","assets":[]}},"slug":"decision-boundaries","timeCommitment":197000},{"id":"XBc9n","name":"Linear classifier model","content":{"typeName":"lecture","definition":{"duration":344000,"videoId":"CbofsdaSEeWgHxKlTdK7JQ","assets":[]}},"slug":"linear-classifier-model","timeCommitment":344000},{"id":"Qy2js","name":"Effect of coefficient values on decision boundary","content":{"typeName":"lecture","definition":{"duration":146000,"videoId":"DcEBSdaSEeWa2w6ZRgn-lw","assets":[]}},"slug":"effect-of-coefficient-values-on-decision-boundary","timeCommitment":146000},{"id":"WHIMY","name":"Using features of the inputs","content":{"typeName":"lecture","definition":{"duration":141000,"videoId":"b2o6UNaSEeWGkg42feWbDw","assets":[]}},"slug":"using-features-of-the-inputs","timeCommitment":141000}],"slug":"linear-classifiers","timeCommitment":1793000},{"id":"PWLfy","name":"Class probabilities","elements":[{"id":"j4Ji0","name":"Predicting class probabilities","content":{"typeName":"lecture","definition":{"duration":97000,"videoId":"b2nsL9aSEeWGkg42feWbDw","assets":[]}},"slug":"predicting-class-probabilities","timeCommitment":97000},{"id":"p6rtM","name":"Review of basics of probabilities","content":{"typeName":"lecture","definition":{"duration":384000,"videoId":"t5jGE9aSEeWl_gqbCJh90Q","assets":[]}},"slug":"review-of-basics-of-probabilities","timeCommitment":384000},{"id":"Cun2N","name":"Review of basics of conditional probabilities","content":{"typeName":"lecture","definition":{"duration":511000,"videoId":"xQt2jtaSEeWgHxKlTdK7JQ","assets":[]}},"slug":"review-of-basics-of-conditional-probabilities","timeCommitment":511000},{"id":"f0nhO","name":"Using probabilities in classification","content":{"typeName":"lecture","definition":{"duration":155000,"videoId":"QJTUCtaTEeWROxIfxROUQw","assets":[]}},"slug":"using-probabilities-in-classification","timeCommitment":155000}],"slug":"class-probabilities","timeCommitment":1147000},{"id":"uxJy2","name":"Logistic regression","elements":[{"id":"OV5Kt","name":"Predicting class probabilities with (generalized) linear models","content":{"typeName":"lecture","definition":{"duration":322000,"videoId":"WV5ZRNaTEeWCpQ7rT5LraQ","assets":[]}},"slug":"predicting-class-probabilities-with-generalized-linear-models","timeCommitment":322000},{"id":"KXvGC","name":"The sigmoid (or logistic) link function","content":{"typeName":"lecture","definition":{"duration":288000,"videoId":"Z5bw-taTEeWl_gqbCJh90Q","assets":[]}},"slug":"the-sigmoid-or-logistic-link-function","timeCommitment":288000},{"id":"OJQXu","name":"Logistic regression model","content":{"typeName":"lecture","definition":{"duration":307000,"videoId":"tyoIt9aTEeWgHxKlTdK7JQ","assets":[]}},"slug":"logistic-regression-model","timeCommitment":307000},{"id":"JkEEH","name":"Effect of coefficient values on predicted probabilities","content":{"typeName":"lecture","definition":{"duration":437000,"videoId":"zAFsj9aTEeWBKwo1vzQ7vQ","assets":[]}},"slug":"effect-of-coefficient-values-on-predicted-probabilities","timeCommitment":437000},{"id":"GuxAJ","name":"Overview of learning logistic regression models","content":{"typeName":"lecture","definition":{"duration":143000,"videoId":"NUplldaUEeWgHxKlTdK7JQ","assets":[]}},"slug":"overview-of-learning-logistic-regression-models","timeCommitment":143000}],"slug":"logistic-regression","timeCommitment":1497000},{"id":"0V9gr","name":"Practical issues for classification","elements":[{"id":"kCY0D","name":"Encoding categorical inputs","content":{"typeName":"lecture","definition":{"duration":290000,"videoId":"U0fbiNaUEeWP4xKEJ_Y9bQ","assets":[]}},"slug":"encoding-categorical-inputs","timeCommitment":290000},{"id":"N7QA6","name":"Multiclass classification with 1 versus all","content":{"typeName":"lecture","definition":{"duration":441000,"videoId":"89cQo9aUEeWCpQ7rT5LraQ","assets":[]}},"slug":"multiclass-classification-with-1-versus-all","timeCommitment":441000}],"slug":"practical-issues-for-classification","timeCommitment":731000},{"id":"bLvD3","name":"Summarizing linear classifiers & logistic regression","elements":[{"id":"laPcB","name":"Recap of logistic regression classifier","content":{"typeName":"lecture","definition":{"duration":86000,"videoId":"YU4YFOI1EeWrggp1qGN5NQ","assets":[]}},"slug":"recap-of-logistic-regression-classifier","timeCommitment":86000},{"id":"cddwS","name":"Linear Classifiers & Logistic Regression","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"k6oa-uA3EeWz2w65RZQpNw@4","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"linear-classifiers-logistic-regression","timeCommitment":600000}],"slug":"summarizing-linear-classifiers-logistic-regression","timeCommitment":686000},{"id":"to8Lc","name":"Programming Assignment","elements":[{"id":"NtMAS","name":"Predicting sentiment from product reviews","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"CjaRHtXFEeWPfAqaKbq0AQ@35"}},"slug":"predicting-sentiment-from-product-reviews","timeCommitment":600000},{"id":"gLYyI","name":"Predicting sentiment from product reviews","content":{"typeName":"exam","definition":{"questionCount":12,"assessmentId":"C_4LFtXFEeWaqBKcouTmNw@20","gradingWeight":6,"passingFraction":0.7,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":12,"learningObjectives":[]}},"slug":"predicting-sentiment-from-product-reviews","timeCommitment":1440000}],"slug":"programming-assignment","timeCommitment":2040000}],"slug":"linear-classifiers-logistic-regression","timeCommitment":7894000},{"id":"GncDF","name":"Learning Linear Classifiers","description":"Once familiar with linear classifiers and logistic regression, you can now dive in and write your first learning algorithm for classification. In particular, you will use gradient ascent to learn the coefficients of your classifier from data. You first will need to define the quality metric for these tasks using an approach called maximum likelihood estimation (MLE). You will also become familiar with a simple technique for selecting the step size for gradient ascent. An optional, advanced part of this module will cover the derivation of the gradient for logistic regression.  You will implement your own learning algorithm for logistic regression from scratch, and use it to learn a sentiment analysis classifier.","elements":[{"id":"nxSR4","name":"Maximum likelihood estimation","elements":[{"id":"OyDce","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"BlrHgdt0EeW2uRJdl6ILzQ@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"uxALW","name":"Goal: Learning parameters of logistic regression","content":{"typeName":"lecture","definition":{"duration":136000,"videoId":"Q8Qaxtf2EeW9UA5SHBd0bQ","assets":[]}},"slug":"goal-learning-parameters-of-logistic-regression","timeCommitment":136000},{"id":"BXKp8","name":"Intuition behind maximum likelihood estimation","content":{"typeName":"lecture","definition":{"duration":244000,"videoId":"VuxuLtf2EeWAqwq9FvxstQ","assets":[]}},"slug":"intuition-behind-maximum-likelihood-estimation","timeCommitment":244000},{"id":"Qjvns","name":"Data likelihood","content":{"typeName":"lecture","definition":{"duration":481000,"videoId":"tfmew9f2EeWtkRKN9hTV7w","assets":[]}},"slug":"data-likelihood","timeCommitment":481000},{"id":"G5kSo","name":"Finding best linear classifier with gradient ascent","content":{"typeName":"lecture","definition":{"duration":202000,"videoId":"v57zSNf2EeWX1RJnGGebvQ","assets":[]}},"slug":"finding-best-linear-classifier-with-gradient-ascent","timeCommitment":202000}],"slug":"maximum-likelihood-estimation","timeCommitment":1663000},{"id":"z8bcQ","name":"Gradient ascent algorithm for learning logistic regression classifier","elements":[{"id":"uwmnm","name":"Review of gradient ascent","content":{"typeName":"lecture","definition":{"duration":377000,"videoId":"E-9IgNf7EeWneA7rltV1xw","assets":[]}},"slug":"review-of-gradient-ascent","timeCommitment":377000},{"id":"dYFZx","name":"Learning algorithm for logistic regression","content":{"typeName":"lecture","definition":{"duration":185000,"videoId":"I9ok49f7EeWKWA5OGVjXfQ","assets":[]}},"slug":"learning-algorithm-for-logistic-regression","timeCommitment":185000},{"id":"UEmJg","name":"Example of computing derivative for logistic regression","content":{"typeName":"lecture","definition":{"duration":358000,"videoId":"NsjyhNf7EeWxMgr486_OKw","assets":[]}},"slug":"example-of-computing-derivative-for-logistic-regression","timeCommitment":358000},{"id":"V9Ox8","name":"Interpreting derivative for logistic regression","content":{"typeName":"lecture","definition":{"duration":337000,"videoId":"R0WGm9f7EeWtkRKN9hTV7w","assets":[]}},"slug":"interpreting-derivative-for-logistic-regression","timeCommitment":337000},{"id":"mMJ2j","name":"Summary of gradient ascent for logistic regression","content":{"typeName":"lecture","definition":{"duration":142000,"videoId":"VU-8M9f7EeWAqwq9FvxstQ","assets":[]}},"slug":"summary-of-gradient-ascent-for-logistic-regression","timeCommitment":142000}],"slug":"gradient-ascent-algorithm-for-learning-logistic-regression-classifier","timeCommitment":1399000},{"id":"IjuDe","name":"Choosing step size for gradient ascent/descent","elements":[{"id":"vFhWy","name":"Choosing step size","content":{"typeName":"lecture","definition":{"duration":358000,"videoId":"YHKvz9f7EeW9UA5SHBd0bQ","assets":[]}},"slug":"choosing-step-size","timeCommitment":358000},{"id":"MVzEy","name":"Careful with step sizes that are too large","content":{"typeName":"lecture","definition":{"duration":264000,"videoId":"a6a6zNf7EeWX1RJnGGebvQ","assets":[]}},"slug":"careful-with-step-sizes-that-are-too-large","timeCommitment":264000},{"id":"hzxFS","name":"Rule of thumb for choosing step size","content":{"typeName":"lecture","definition":{"duration":212000,"videoId":"doBwW9f7EeW9UA5SHBd0bQ","assets":[]}},"slug":"rule-of-thumb-for-choosing-step-size","timeCommitment":212000}],"slug":"choosing-step-size-for-gradient-ascent-descent","timeCommitment":834000},{"id":"c7Di7","name":"(VERY OPTIONAL LESSON) Deriving gradient of logistic regression","elements":[{"id":"QcofN","name":"(VERY OPTIONAL) Deriving gradient of logistic regression: Log trick","content":{"typeName":"lecture","definition":{"duration":298000,"videoId":"h0PR9df7EeWtkRKN9hTV7w","assets":[]}},"slug":"very-optional-deriving-gradient-of-logistic-regression-log-trick","timeCommitment":298000},{"id":"1ZeTC","name":"(VERY OPTIONAL) Expressing the log-likelihood","content":{"typeName":"lecture","definition":{"duration":183000,"videoId":"nVJ89df7EeWKWA5OGVjXfQ","assets":[]}},"slug":"very-optional-expressing-the-log-likelihood","timeCommitment":183000},{"id":"hKVpr","name":"(VERY OPTIONAL) Deriving probability y=-1 given x","content":{"typeName":"lecture","definition":{"duration":127000,"videoId":"rnCBRtf7EeW9UA5SHBd0bQ","assets":[]}},"slug":"very-optional-deriving-probability-y-1-given-x","timeCommitment":127000},{"id":"Ageum","name":"(VERY OPTIONAL) Rewriting the log likelihood into a simpler form","content":{"typeName":"lecture","definition":{"duration":489000,"videoId":"vDp5UNf7EeWneA7rltV1xw","assets":[]}},"slug":"very-optional-rewriting-the-log-likelihood-into-a-simpler-form","timeCommitment":489000},{"id":"W3VBS","name":"(VERY OPTIONAL) Deriving gradient of log likelihood","content":{"typeName":"lecture","definition":{"duration":481000,"videoId":"dW0Pbtf8EeWYtBJcYCfJww","assets":[]}},"slug":"very-optional-deriving-gradient-of-log-likelihood","timeCommitment":481000}],"slug":"very-optional-lesson-deriving-gradient-of-logistic-regression","timeCommitment":1578000},{"id":"Cnumr","name":"Summarizing learning linear classifiers","elements":[{"id":"A0OjE","name":"Recap of learning logistic regression classifiers","content":{"typeName":"lecture","definition":{"duration":103000,"videoId":"fywsnNf8EeW9UA5SHBd0bQ","assets":[]}},"slug":"recap-of-learning-logistic-regression-classifiers","timeCommitment":103000},{"id":"5MWIa","name":"Learning Linear Classifiers","content":{"typeName":"exam","definition":{"questionCount":6,"assessmentId":"H2iIIuA5EeW0ggpqoQox3w@8","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":6,"learningObjectives":[]}},"slug":"learning-linear-classifiers","timeCommitment":720000}],"slug":"summarizing-learning-linear-classifiers","timeCommitment":823000},{"id":"sDhRI","name":"Programming Assignment","elements":[{"id":"zU6HO","name":"Implementing logistic regression from scratch","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"nFXFs9mGEeWtkRKN9hTV7w@47"}},"slug":"implementing-logistic-regression-from-scratch","timeCommitment":600000},{"id":"eHHDd","name":"Implementing logistic regression from scratch","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"qC4ATtmGEeWtkRKN9hTV7w@10","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":8,"learningObjectives":[]}},"slug":"implementing-logistic-regression-from-scratch","timeCommitment":960000}],"slug":"programming-assignment","timeCommitment":1560000}],"slug":"learning-linear-classifiers","timeCommitment":7857000},{"id":"Ai1YN","name":"Overfitting & Regularization in Logistic Regression","description":"As we saw in the regression course, overfitting is perhaps the most significant challenge you will face as you apply machine learning approaches in practice. This challenge can be particularly significant for logistic regression, as you will discover in this module, since we not only risk getting an overly complex decision boundary, but your classifier can also become overly confident about the probabilities it predicts. In this module, you will investigate overfitting in classification in significant detail, and obtain broad practical insights from some interesting visualizations of the classifiers' outputs. You will then add a regularization term to your optimization to mitigate overfitting. You will investigate both L2 regularization to penalize large coefficient values, and L1 regularization to obtain additional sparsity in the coefficients. Finally, you will modify your gradient ascent algorithm to learn regularized logistic regression classifiers. You will implement your own regularized logistic regression classifier from scratch, and investigate the impact of the L2 penalty on real-world sentiment analysis data.","elements":[{"id":"u8Ont","name":"Overfitting in classification","elements":[{"id":"5x4i6","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"WfsEE9t0EeWu5xLV6iKhSQ@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"RzxaQ","name":"Evaluating a classifier","content":{"typeName":"lecture","definition":{"duration":201000,"videoId":"AsvbQNkLEeWKWA5OGVjXfQ","assets":[]}},"slug":"evaluating-a-classifier","timeCommitment":201000},{"id":"WZJC0","name":"Review of overfitting in regression","content":{"typeName":"lecture","definition":{"duration":198000,"videoId":"GZZcJNkLEeWAqwq9FvxstQ","assets":[]}},"slug":"review-of-overfitting-in-regression","timeCommitment":198000},{"id":"V1hPz","name":"Overfitting in classification","content":{"typeName":"lecture","definition":{"duration":340000,"videoId":"K-6O3dkLEeW9UA5SHBd0bQ","assets":[]}},"slug":"overfitting-in-classification","timeCommitment":340000},{"id":"BSNGu","name":"Visualizing overfitting with high-degree polynomial features","content":{"typeName":"lecture","definition":{"duration":235000,"videoId":"OQBZ_9kLEeWtkRKN9hTV7w","assets":[]}},"slug":"visualizing-overfitting-with-high-degree-polynomial-features","timeCommitment":235000}],"slug":"overfitting-in-classification","timeCommitment":1574000},{"id":"PzTO1","name":"Overconfident predictions due to overfitting","elements":[{"id":"FIYUC","name":"Overfitting in classifiers leads to overconfident predictions","content":{"typeName":"lecture","definition":{"duration":300000,"videoId":"mZbDCNkLEeWKWA5OGVjXfQ","assets":[]}},"slug":"overfitting-in-classifiers-leads-to-overconfident-predictions","timeCommitment":300000},{"id":"jIjw5","name":"Visualizing overconfident predictions","content":{"typeName":"lecture","definition":{"duration":253000,"videoId":"tvqQTdkLEeWYtBJcYCfJww","assets":[]}},"slug":"visualizing-overconfident-predictions","timeCommitment":253000},{"id":"lzGuI","name":"(OPTIONAL) Another perspecting on overfitting in logistic regression","content":{"typeName":"lecture","definition":{"duration":517000,"videoId":"15vVm9kLEeWX1RJnGGebvQ","assets":[]}},"slug":"optional-another-perspecting-on-overfitting-in-logistic-regression","timeCommitment":517000}],"slug":"overconfident-predictions-due-to-overfitting","timeCommitment":1070000},{"id":"ruZPz","name":"L2 regularized logistic regression","elements":[{"id":"7Do2G","name":"Penalizing large coefficients to mitigate overfitting","content":{"typeName":"lecture","definition":{"duration":312000,"videoId":"5q_aZdkLEeWdIgr37d1Ygw","assets":[]}},"slug":"penalizing-large-coefficients-to-mitigate-overfitting","timeCommitment":312000},{"id":"DBTNt","name":"L2 regularized logistic regression","content":{"typeName":"lecture","definition":{"duration":291000,"videoId":"8fOb19kLEeWYtBJcYCfJww","assets":[]}},"slug":"l2-regularized-logistic-regression","timeCommitment":291000},{"id":"1VXLD","name":"Visualizing effect of L2 regularization in logistic regression","content":{"typeName":"lecture","definition":{"duration":345000,"videoId":"CVOmiNkMEeWneA7rltV1xw","assets":[]}},"slug":"visualizing-effect-of-l2-regularization-in-logistic-regression","timeCommitment":345000},{"id":"4JxyQ","name":"Learning L2 regularized logistic regression with gradient ascent","content":{"typeName":"lecture","definition":{"duration":455000,"videoId":"MrRNFdkMEeWKWA5OGVjXfQ","assets":[]}},"slug":"learning-l2-regularized-logistic-regression-with-gradient-ascent","timeCommitment":455000}],"slug":"l2-regularized-logistic-regression","timeCommitment":1403000},{"id":"iFj2o","name":"Sparse logistic regression","elements":[{"id":"ypt7u","name":"Sparse logistic regression with L1 regularization","content":{"typeName":"lecture","definition":{"duration":453000,"videoId":"P2xfV9kMEeWAqwq9FvxstQ","assets":[]}},"slug":"sparse-logistic-regression-with-l1-regularization","timeCommitment":453000}],"slug":"sparse-logistic-regression","timeCommitment":453000},{"id":"w2KQf","name":"Summarizing overfitting & regularization in logistic regression","elements":[{"id":"YYGdW","name":"Recap of overfitting & regularization in logistic regression","content":{"typeName":"lecture","definition":{"duration":58000,"videoId":"80UstdkMEeWneA7rltV1xw","assets":[]}},"slug":"recap-of-overfitting-regularization-in-logistic-regression","timeCommitment":58000},{"id":"jp0Yp","name":"Overfitting & Regularization in Logistic Regression","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"U-_kSuA_EeWbuA75seJe7Q@9","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":8,"learningObjectives":[]}},"slug":"overfitting-regularization-in-logistic-regression","timeCommitment":960000}],"slug":"summarizing-overfitting-regularization-in-logistic-regression","timeCommitment":1018000},{"id":"9nKtJ","name":"Programming Assignment","elements":[{"id":"diZFx","name":"Logistic Regression with L2 regularization","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"0ajlidmGEeWxMgr486_OKw@36"}},"slug":"logistic-regression-with-l2-regularization","timeCommitment":600000},{"id":"bpHt8","name":"Logistic Regression with L2 regularization","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"hmfz1dmHEeWxMgr486_OKw@11","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":8,"learningObjectives":[]}},"slug":"logistic-regression-with-l2-regularization","timeCommitment":960000}],"slug":"programming-assignment","timeCommitment":1560000}],"slug":"overfitting-regularization-in-logistic-regression","timeCommitment":7078000},{"id":"7oZfi","name":"Decision Trees","description":"Along with linear classifiers, decision trees are amongst the most widely used classification techniques in the real world. This method is extremely intuitive, simple to implement and provides interpretable predictions. In this module, you will become familiar with the core decision trees representation. You will then design a simple, recursive greedy algorithm to learn decision trees from data. Finally, you will extend this approach to deal with continuous inputs, a fundamental requirement for practical problems. In this module, you will investigate a brand new case-study in the financial sector: predicting the risk associated with a bank loan. You will implement your own decision tree learning algorithm on real loan data.","elements":[{"id":"Qh0Ql","name":"Intuition behind decision trees","elements":[{"id":"2PWpF","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"hoPsXtt0EeWirA70Ynl3tQ@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"ZtvkP","name":"Predicting loan defaults with decision trees","content":{"typeName":"lecture","definition":{"duration":230000,"videoId":"CDJw89aWEeWl_gqbCJh90Q","assets":[]}},"slug":"predicting-loan-defaults-with-decision-trees","timeCommitment":230000},{"id":"F8kuT","name":"Intuition behind decision trees","content":{"typeName":"lecture","definition":{"duration":105000,"videoId":"Bsar29aWEeWl_gqbCJh90Q","assets":[]}},"slug":"intuition-behind-decision-trees","timeCommitment":105000},{"id":"xilmJ","name":"Task of learning decision trees from data","content":{"typeName":"lecture","definition":{"duration":181000,"videoId":"MzDpT9aWEeWf3Qr1lQJBTw","assets":[]}},"slug":"task-of-learning-decision-trees-from-data","timeCommitment":181000}],"slug":"intuition-behind-decision-trees","timeCommitment":1116000},{"id":"sQ7ne","name":"Learning decision trees","elements":[{"id":"Oor8r","name":"Recursive greedy algorithm","content":{"typeName":"lecture","definition":{"duration":252000,"videoId":"UbbJNNaWEeWBKwo1vzQ7vQ","assets":[]}},"slug":"recursive-greedy-algorithm","timeCommitment":252000},{"id":"U7PcP","name":"Learning a decision stump","content":{"typeName":"lecture","definition":{"duration":219000,"videoId":"bjuBt9aWEeWl_gqbCJh90Q","assets":[]}},"slug":"learning-a-decision-stump","timeCommitment":219000},{"id":"9RN9F","name":"Selecting best feature to split on","content":{"typeName":"lecture","definition":{"duration":381000,"videoId":"mNHpTtaWEeWGkg42feWbDw","assets":[]}},"slug":"selecting-best-feature-to-split-on","timeCommitment":381000},{"id":"fTlJU","name":"When to stop recursing","content":{"typeName":"lecture","definition":{"duration":260000,"videoId":"qJ5n29aWEeWCpQ7rT5LraQ","assets":[]}},"slug":"when-to-stop-recursing","timeCommitment":260000}],"slug":"learning-decision-trees","timeCommitment":1112000},{"id":"kf8OT","name":"Using the learned decision tree","elements":[{"id":"HM4VD","name":"Making predictions with decision trees","content":{"typeName":"lecture","definition":{"duration":76000,"videoId":"xas7lNaWEeWP4xKEJ_Y9bQ","assets":[]}},"slug":"making-predictions-with-decision-trees","timeCommitment":76000},{"id":"IVMdN","name":"Multiclass classification with decision trees","content":{"typeName":"lecture","definition":{"duration":159000,"videoId":"KbGed9aXEeWWtQqDItCsNQ","assets":[]}},"slug":"multiclass-classification-with-decision-trees","timeCommitment":159000}],"slug":"using-the-learned-decision-tree","timeCommitment":235000},{"id":"rG6ux","name":"Learning decision trees with continuous inputs","elements":[{"id":"tn6M9","name":"Threshold splits for continuous inputs","content":{"typeName":"lecture","definition":{"duration":373000,"videoId":"v1qDf9dKEeWXSxI4GxQ8ow","assets":[]}},"slug":"threshold-splits-for-continuous-inputs","timeCommitment":373000},{"id":"sKrGp","name":"(OPTIONAL) Picking the best threshold to split on","content":{"typeName":"lecture","definition":{"duration":183000,"videoId":"5ZRunNdKEeWstRLAuwLDKQ","assets":[]}},"slug":"optional-picking-the-best-threshold-to-split-on","timeCommitment":183000},{"id":"kyi11","name":"Visualizing decision boundaries","content":{"typeName":"lecture","definition":{"duration":336000,"videoId":"9QIsIddKEeWXSxI4GxQ8ow","assets":[]}},"slug":"visualizing-decision-boundaries","timeCommitment":336000}],"slug":"learning-decision-trees-with-continuous-inputs","timeCommitment":892000},{"id":"y4T9K","name":"Summarizing decision trees","elements":[{"id":"ROzb2","name":"Recap of decision trees","content":{"typeName":"lecture","definition":{"duration":56000,"videoId":"Bpi8nddLEeWXiQ5j8ahyJQ","assets":[]}},"slug":"recap-of-decision-trees","timeCommitment":56000},{"id":"c4ssq","name":"Decision Trees","content":{"typeName":"exam","definition":{"questionCount":11,"assessmentId":"C8Y88OCYEeWYKQrNHa0UDw@4","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":11,"learningObjectives":[]}},"slug":"decision-trees","timeCommitment":1320000}],"slug":"summarizing-decision-trees","timeCommitment":1376000},{"id":"oRHVl","name":"Programming Assignment 1","elements":[{"id":"ssPTc","name":"Identifying safe loans with decision trees","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"1FClM9oDEeWXuxJgUJEB-Q@27"}},"slug":"identifying-safe-loans-with-decision-trees","timeCommitment":600000},{"id":"xHRYe","name":"Identifying safe loans with decision trees","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"3bwkntoDEeWXuxJgUJEB-Q@11","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":7,"learningObjectives":[]}},"slug":"identifying-safe-loans-with-decision-trees","timeCommitment":840000}],"slug":"programming-assignment-1","timeCommitment":1440000},{"id":"k1Xkv","name":"Programming Assignment 2","elements":[{"id":"seWHJ","name":"Implementing binary decision trees","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"2o67JtkxEeW9UA5SHBd0bQ@32"}},"slug":"implementing-binary-decision-trees","timeCommitment":600000},{"id":"WxrJw","name":"Implementing binary decision trees","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"52x-SdkxEeWxMgr486_OKw@7","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":7,"learningObjectives":[]}},"slug":"implementing-binary-decision-trees","timeCommitment":840000}],"slug":"programming-assignment-2","timeCommitment":1440000}],"slug":"decision-trees","timeCommitment":7611000},{"id":"ahoC8","name":"Preventing Overfitting in Decision Trees","description":"Out of all machine learning techniques, decision trees are amongst the most prone to overfitting. No practical implementation is possible without including approaches that mitigate this challenge. In this module, through various visualizations and investigations, you will investigate why decision trees suffer from significant overfitting problems. Using the principle of Occam's razor, you will mitigate overfitting by learning simpler trees. At first, you will design algorithms that stop the learning process before the decision trees become overly complex. In an optional segment, you will design a very practical approach that learns an overly-complex tree, and then simplifies it with pruning. Your implementation will investigate the effect of these techniques on mitigating overfitting on our real-world loan data set. ","elements":[{"id":"Uy4yT","name":"Overfitting in decision trees","elements":[{"id":"JhRwM","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"PBF2IttyEeWM0A55yw9aZQ@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"czRmA","name":"A review of overfitting","content":{"typeName":"lecture","definition":{"duration":164000,"videoId":"JqCGRNf9EeWX1RJnGGebvQ","assets":[]}},"slug":"a-review-of-overfitting","timeCommitment":164000},{"id":"XcPVL","name":"Overfitting in decision trees","content":{"typeName":"lecture","definition":{"duration":353000,"videoId":"NZWQJtf9EeWX1RJnGGebvQ","assets":[]}},"slug":"overfitting-in-decision-trees","timeCommitment":353000}],"slug":"overfitting-in-decision-trees","timeCommitment":1117000},{"id":"2CW85","name":"Early stopping to avoid overfitting","elements":[{"id":"tUvBS","name":"Principle of Occam's razor: Learning simpler decision trees","content":{"typeName":"lecture","definition":{"duration":316000,"videoId":"S2IB2Nf9EeWxMgr486_OKw","assets":[]}},"slug":"principle-of-occams-razor-learning-simpler-decision-trees","timeCommitment":316000},{"id":"gCuZ8","name":"Early stopping in learning decision trees","content":{"typeName":"lecture","definition":{"duration":413000,"videoId":"dLbpMtf9EeWdIgr37d1Ygw","assets":[]}},"slug":"early-stopping-in-learning-decision-trees","timeCommitment":413000}],"slug":"early-stopping-to-avoid-overfitting","timeCommitment":729000},{"id":"crjEu","name":"(OPTIONAL LESSON) Pruning decision trees","elements":[{"id":"9nMdb","name":"(OPTIONAL) Motivating pruning","content":{"typeName":"lecture","definition":{"duration":491000,"videoId":"g8qeodf9EeWdIgr37d1Ygw","assets":[]}},"slug":"optional-motivating-pruning","timeCommitment":491000},{"id":"qvf6v","name":"(OPTIONAL) Pruning decision trees to avoid overfitting","content":{"typeName":"lecture","definition":{"duration":374000,"videoId":"mDTBXdf9EeWdIgr37d1Ygw","assets":[]}},"slug":"optional-pruning-decision-trees-to-avoid-overfitting","timeCommitment":374000},{"id":"wmODB","name":"(OPTIONAL) Tree pruning algorithm","content":{"typeName":"lecture","definition":{"duration":228000,"videoId":"ppxZYNf9EeW9UA5SHBd0bQ","assets":[]}},"slug":"optional-tree-pruning-algorithm","timeCommitment":228000}],"slug":"optional-lesson-pruning-decision-trees","timeCommitment":1093000},{"id":"rSwoy","name":"Summarizing preventing overfitting in decision trees","elements":[{"id":"bRwHo","name":"Recap of overfitting and regularization in decision trees","content":{"typeName":"lecture","definition":{"duration":72000,"videoId":"uiFyeNf9EeWX1RJnGGebvQ","assets":[]}},"slug":"recap-of-overfitting-and-regularization-in-decision-trees","timeCommitment":72000},{"id":"NDTdJ","name":"Preventing Overfitting in Decision Trees","content":{"typeName":"exam","definition":{"questionCount":11,"assessmentId":"0Enz3eCeEeW6uRIZVrI4HQ@4","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":11,"learningObjectives":[]}},"slug":"preventing-overfitting-in-decision-trees","timeCommitment":1320000}],"slug":"summarizing-preventing-overfitting-in-decision-trees","timeCommitment":1392000},{"id":"EpInd","name":"Programming Assignment","elements":[{"id":"AqDoX","name":"Decision Trees in Practice","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"UaZUgdk0EeWYtBJcYCfJww@24"}},"slug":"decision-trees-in-practice","timeCommitment":600000},{"id":"xRbhG","name":"Decision Trees in Practice","content":{"typeName":"exam","definition":{"questionCount":14,"assessmentId":"WVFfndk0EeWX1RJnGGebvQ@15","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":14,"learningObjectives":[]}},"slug":"decision-trees-in-practice","timeCommitment":1680000}],"slug":"programming-assignment","timeCommitment":2280000}],"slug":"preventing-overfitting-in-decision-trees","timeCommitment":6611000},{"id":"CPMVO","name":"Handling Missing Data","description":"Real-world machine learning problems are fraught with missing data. That is, very often, some of the inputs are not observed for all data points. This challenge is very significant, happens in most cases, and needs to be addressed carefully to obtain great performance. And, this issue is rarely discussed in machine learning courses. In this module, you will tackle the missing data challenge head on. You will start with the two most basic techniques to convert a dataset with missing data into a clean dataset, namely skipping missing values and inputing missing values. In an advanced section, you will also design a modification of the decision tree learning algorithm that builds decisions about missing data right into the model. You will also explore these techniques in your real-data implementation.  ","elements":[{"id":"xjLD2","name":"Basic strategies for handling missing data","elements":[{"id":"zzmdu","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"jriruNtyEeWDfA5_5YqObw@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"VtYde","name":"Challenge of missing data","content":{"typeName":"lecture","definition":{"duration":230000,"videoId":"YgyZn9dLEeWWtQqDItCsNQ","assets":[]}},"slug":"challenge-of-missing-data","timeCommitment":230000},{"id":"RUkgm","name":"Strategy 1: Purification by skipping missing data","content":{"typeName":"lecture","definition":{"duration":244000,"videoId":"Z5bnj9dLEeWyQA52VAEGGw","assets":[]}},"slug":"strategy-1-purification-by-skipping-missing-data","timeCommitment":244000},{"id":"4afWj","name":"Strategy 2: Purification by imputing missing data","content":{"typeName":"lecture","definition":{"duration":285000,"videoId":"n_2vZddLEeWXiQ5j8ahyJQ","assets":[]}},"slug":"strategy-2-purification-by-imputing-missing-data","timeCommitment":285000}],"slug":"basic-strategies-for-handling-missing-data","timeCommitment":1359000},{"id":"Fg9kx","name":"Strategy 3: Modify learning algorithm to explicitly handle missing data","elements":[{"id":"rsIr1","name":"Modifying decision trees to handle missing data","content":{"typeName":"lecture","definition":{"duration":289000,"videoId":"pTzq79dLEeWJRxKPFm1zbQ","assets":[]}},"slug":"modifying-decision-trees-to-handle-missing-data","timeCommitment":289000},{"id":"ruaJi","name":"Feature split selection with missing data","content":{"typeName":"lecture","definition":{"duration":335000,"videoId":"5vR_OtdLEeWYpwrnHobp5Q","assets":[]}},"slug":"feature-split-selection-with-missing-data","timeCommitment":335000}],"slug":"strategy-3-modify-learning-algorithm-to-explicitly-handle-missing-data","timeCommitment":624000},{"id":"jAHfs","name":"Summarizing handling missing data","elements":[{"id":"f6DBX","name":"Recap of handling missing data","content":{"typeName":"lecture","definition":{"duration":95000,"videoId":"3DThHddLEeWXiQ5j8ahyJQ","assets":[]}},"slug":"recap-of-handling-missing-data","timeCommitment":95000},{"id":"fEDCt","name":"Handling Missing Data","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"shu_LuCgEeWEOQoxPdNRWw@2","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"handling-missing-data","timeCommitment":840000}],"slug":"summarizing-handling-missing-data","timeCommitment":935000}],"slug":"handling-missing-data","timeCommitment":2918000},{"id":"s7KRa","name":"Boosting","description":"One of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly accurate ensemble. This question lead to the developing of boosting, one of the most important and practical techniques in machine learning today. This simple approach can boost the accuracy of any classifier, and is widely used in practice, e.g., it's used by more than half of the teams who win the Kaggle machine learning competitions. In this module, you will first define the ensemble classifier, where multiple models vote on the best prediction. You will then explore a boosting algorithm called  AdaBoost, which provides a great approach for boosting classifiers. Through visualizations, you will become familiar with many of the practical aspects of this techniques. You will create your very own implementation of AdaBoost, from scratch, and use it to boost the performance of your loan risk predictor on real data. ","elements":[{"id":"1MCjF","name":"The amazing idea of boosting a classifier","elements":[{"id":"07FKm","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"4pfFx9tyEeWu5xLV6iKhSQ@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"3ywWA","name":"The boosting question","content":{"typeName":"lecture","definition":{"duration":224000,"videoId":"HsVIUNdMEeWWtQqDItCsNQ","assets":[]}},"slug":"the-boosting-question","timeCommitment":224000},{"id":"IAous","name":"Ensemble classifiers","content":{"typeName":"lecture","definition":{"duration":329000,"videoId":"MUMEyddMEeWXSxI4GxQ8ow","assets":[]}},"slug":"ensemble-classifiers","timeCommitment":329000},{"id":"rV0iX","name":"Boosting","content":{"typeName":"lecture","definition":{"duration":358000,"videoId":"WdWavtg3EeWX1RJnGGebvQ","assets":[]}},"slug":"boosting","timeCommitment":358000}],"slug":"the-amazing-idea-of-boosting-a-classifier","timeCommitment":1511000},{"id":"DRAjs","name":"AdaBoost","elements":[{"id":"oyMBd","name":"AdaBoost overview","content":{"typeName":"lecture","definition":{"duration":191000,"videoId":"ddutwdg3EeWneA7rltV1xw","assets":[]}},"slug":"adaboost-overview","timeCommitment":191000},{"id":"vR1RK","name":"Weighted error","content":{"typeName":"lecture","definition":{"duration":294000,"videoId":"HXH33ddNEeWyQA52VAEGGw","assets":[]}},"slug":"weighted-error","timeCommitment":294000},{"id":"t4tAK","name":"Computing coefficient of each ensemble component","content":{"typeName":"lecture","definition":{"duration":293000,"videoId":"LWaX0NdNEeWstRLAuwLDKQ","assets":[]}},"slug":"computing-coefficient-of-each-ensemble-component","timeCommitment":293000},{"id":"in5T5","name":"Reweighing data to focus on mistakes","content":{"typeName":"lecture","definition":{"duration":295000,"videoId":"W-LFc9dNEeWWtQqDItCsNQ","assets":[]}},"slug":"reweighing-data-to-focus-on-mistakes","timeCommitment":295000},{"id":"LqKAF","name":"Normalizing weights","content":{"typeName":"lecture","definition":{"duration":126000,"videoId":"eoKVZNdNEeWJRxKPFm1zbQ","assets":[]}},"slug":"normalizing-weights","timeCommitment":126000}],"slug":"adaboost","timeCommitment":1199000},{"id":"cZPs3","name":"Applying AdaBoost","elements":[{"id":"um0cX","name":"Example of AdaBoost in action","content":{"typeName":"lecture","definition":{"duration":306000,"videoId":"uENz3tdNEeWZuwofVDUshQ","assets":[]}},"slug":"example-of-adaboost-in-action","timeCommitment":306000},{"id":"bx5YA","name":"Learning boosted decision stumps with AdaBoost","content":{"typeName":"lecture","definition":{"duration":241000,"videoId":"0l70utdNEeWYpwrnHobp5Q","assets":[]}},"slug":"learning-boosted-decision-stumps-with-adaboost","timeCommitment":241000}],"slug":"applying-adaboost","timeCommitment":547000},{"id":"cxLZY","name":"Programming Assignment 1","elements":[{"id":"aMrDF","name":"Exploring Ensemble Methods","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"yENsr9oNEeWgOApRIOF5pQ@23"}},"slug":"exploring-ensemble-methods","timeCommitment":600000},{"id":"FgzAt","name":"Exploring Ensemble Methods","content":{"typeName":"exam","definition":{"questionCount":9,"assessmentId":"yn4aXtoNEeWXuxJgUJEB-Q@8","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":9,"learningObjectives":[]}},"slug":"exploring-ensemble-methods","timeCommitment":1080000}],"slug":"programming-assignment-1","timeCommitment":1680000},{"id":"5dw2v","name":"Convergence and overfitting in boosting","elements":[{"id":"hvl3Z","name":"The Boosting Theorem","content":{"typeName":"lecture","definition":{"duration":236000,"videoId":"6FfN3tdNEeWYpwrnHobp5Q","assets":[]}},"slug":"the-boosting-theorem","timeCommitment":236000},{"id":"8SGv8","name":"Overfitting in boosting","content":{"typeName":"lecture","definition":{"duration":335000,"videoId":"PKkJU9dOEeWYpwrnHobp5Q","assets":[]}},"slug":"overfitting-in-boosting","timeCommitment":335000}],"slug":"convergence-and-overfitting-in-boosting","timeCommitment":571000},{"id":"UEAuD","name":"Summarizing boosting","elements":[{"id":"bQqTw","name":"Ensemble methods, impact of boosting & quick recap","content":{"typeName":"lecture","definition":{"duration":259000,"videoId":"XVs_h9dOEeWYpwrnHobp5Q","assets":[]}},"slug":"ensemble-methods-impact-of-boosting-quick-recap","timeCommitment":259000},{"id":"UTPiQ","name":"Boosting","content":{"typeName":"exam","definition":{"questionCount":11,"assessmentId":"NTNAXOBDEeWbuA75seJe7Q@4","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":11,"learningObjectives":[]}},"slug":"boosting","timeCommitment":1320000}],"slug":"summarizing-boosting","timeCommitment":1579000},{"id":"Jn8kX","name":"Programming Assignment 2","elements":[{"id":"3TYwk","name":"Boosting a decision stump","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"6knS7tnbEeWMwg7LWGlGpQ@24"}},"slug":"boosting-a-decision-stump","timeCommitment":600000},{"id":"0Z2ax","name":"Boosting a decision stump","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"EntwCdncEeWPHw6r45-nxw@28","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":5,"learningObjectives":[]}},"slug":"boosting-a-decision-stump","timeCommitment":600000}],"slug":"programming-assignment-2","timeCommitment":1200000}],"slug":"boosting","timeCommitment":8287000},{"id":"ddJSf","name":"Precision-Recall","description":"In many real-world settings, accuracy or error are not the best quality metrics for classification. You will explore a case-study that significantly highlights this issue: using sentiment analysis to display positive reviews on a restaurant website. Instead of accuracy, you will define two metrics: precision and recall, which are widely used in real-world applications to measure the quality of classifiers. You will explore how the probabilities output by your classifier can be used to trade-off precision with recall, and dive into this spectrum, using precision-recall curves. In your hands-on implementation, you will compute these metrics with your learned classifier on real-world sentiment analysis data.","elements":[{"id":"N2N0y","name":"Why use precision & recall as quality metrics","elements":[{"id":"5PJ3W","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"ISCuqttzEeWo4AonWpaZRw@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"zOBTx","name":"Case-study where accuracy is not best metric for classification","content":{"typeName":"lecture","definition":{"duration":214000,"videoId":"9wTCQdf9EeWKWA5OGVjXfQ","assets":[]}},"slug":"case-study-where-accuracy-is-not-best-metric-for-classification","timeCommitment":214000},{"id":"fUFNh","name":"What is good performance for a classifier?","content":{"typeName":"lecture","definition":{"duration":231000,"videoId":"_wcVAtf9EeWKWA5OGVjXfQ","assets":[]}},"slug":"what-is-good-performance-for-a-classifier","timeCommitment":231000}],"slug":"why-use-precision-recall-as-quality-metrics","timeCommitment":1045000},{"id":"rKDBQ","name":"Precision & recall explained","elements":[{"id":"BrtUG","name":"Precision: Fraction of positive predictions that are actually positive","content":{"typeName":"lecture","definition":{"duration":347000,"videoId":"B2sPj9f-EeWKWA5OGVjXfQ","assets":[]}},"slug":"precision-fraction-of-positive-predictions-that-are-actually-positive","timeCommitment":347000},{"id":"pNTaV","name":"Recall: Fraction of positive data predicted to be positive","content":{"typeName":"lecture","definition":{"duration":199000,"videoId":"afgKHdgEEeWneA7rltV1xw","assets":[]}},"slug":"recall-fraction-of-positive-data-predicted-to-be-positive","timeCommitment":199000}],"slug":"precision-recall-explained","timeCommitment":546000},{"id":"ZsAA1","name":"The precision-recall tradeoff","elements":[{"id":"eej7H","name":"Precision-recall extremes","content":{"typeName":"lecture","definition":{"duration":159000,"videoId":"LgYGrtf-EeWAqwq9FvxstQ","assets":[]}},"slug":"precision-recall-extremes","timeCommitment":159000},{"id":"IMHs2","name":"Trading off precision and recall","content":{"typeName":"lecture","definition":{"duration":294000,"videoId":"OJhTKdf-EeW9UA5SHBd0bQ","assets":[]}},"slug":"trading-off-precision-and-recall","timeCommitment":294000},{"id":"rENu8","name":"Precision-recall curve","content":{"typeName":"lecture","definition":{"duration":336000,"videoId":"P7iqctf-EeWX1RJnGGebvQ","assets":[]}},"slug":"precision-recall-curve","timeCommitment":336000}],"slug":"the-precision-recall-tradeoff","timeCommitment":789000},{"id":"zZ97v","name":"Summarizing precision-recall","elements":[{"id":"ZEQ0e","name":"Recap of precision-recall","content":{"typeName":"lecture","definition":{"duration":85000,"videoId":"R_LW49f-EeWdIgr37d1Ygw","assets":[]}},"slug":"recap-of-precision-recall","timeCommitment":85000},{"id":"pGfWZ","name":"Precision-Recall","content":{"typeName":"exam","definition":{"questionCount":9,"assessmentId":"s2m48uBKEeWk3QqyTSxd4Q@4","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":9,"learningObjectives":[]}},"slug":"precision-recall","timeCommitment":1080000}],"slug":"summarizing-precision-recall","timeCommitment":1165000},{"id":"GxR4J","name":"Programming Assignment","elements":[{"id":"6DQQh","name":"Exploring precision and recall","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"oFhtZ9mHEeWxMgr486_OKw@24"}},"slug":"exploring-precision-and-recall","timeCommitment":600000},{"id":"ObhEq","name":"Exploring precision and recall","content":{"typeName":"exam","definition":{"questionCount":13,"assessmentId":"pLen9dmHEeW9UA5SHBd0bQ@13","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":13,"learningObjectives":[]}},"slug":"exploring-precision-and-recall","timeCommitment":1560000}],"slug":"programming-assignment","timeCommitment":2160000}],"slug":"precision-recall","timeCommitment":5705000},{"id":"6OP8D","name":"Scaling to Huge Datasets & Online Learning","description":"With the advent of the internet, the growth of social media, and the embedding of sensors in the world, the magnitudes of data that our machine learning algorithms must handle have grown tremendously over the last decade. This effect is sometimes called \"Big Data\". Thus, our learning algorithms must scale to bigger and bigger datasets. In this module, you will develop a small modification of gradient ascent called stochastic gradient, which provides significant speedups in the running time of our algorithms. This simple change can drastically improve scaling, but makes the algorithm less stable and harder to use in practice. In this module, you will investigate the practical techniques needed to make stochastic gradient viable, and to thus to obtain learning algorithms that scale to huge datasets. You will also address a new kind of machine learning problem, online learning, where the data streams in over time, and we must learn the coefficients as the data arrives. This task can also be solved with stochastic gradient. You will implement your very own stochastic gradient ascent algorithm for logistic regression from scratch, and evaluate it on sentiment analysis data. ","elements":[{"id":"gZn3x","name":"Scaling ML to huge datasets","elements":[{"id":"rC1np","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"WQ2LidtzEeWXsgrwJZJkXQ@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"Um6kI","name":"Gradient ascent won't scale to today's huge datasets","content":{"typeName":"lecture","definition":{"duration":202000,"videoId":"NkZf79piEeWgOApRIOF5pQ","assets":[]}},"slug":"gradient-ascent-won-t-scale-to-todays-huge-datasets","timeCommitment":202000},{"id":"vEF6f","name":"Timeline of scalable machine learning & stochastic gradient","content":{"typeName":"lecture","definition":{"duration":262000,"videoId":"SeSm9tpiEeWgOApRIOF5pQ","assets":[]}},"slug":"timeline-of-scalable-machine-learning-stochastic-gradient","timeCommitment":262000}],"slug":"scaling-ml-to-huge-datasets","timeCommitment":1064000},{"id":"N8sKL","name":"Scaling ML with stochastic gradient","elements":[{"id":"9dluv","name":"Why gradient ascent won't scale","content":{"typeName":"lecture","definition":{"duration":230000,"videoId":"Y2XjJNpiEeWPHw6r45-nxw","assets":[]}},"slug":"why-gradient-ascent-won-t-scale","timeCommitment":230000},{"id":"d4rdu","name":"Stochastic gradient: Learning one data point at a time","content":{"typeName":"lecture","definition":{"duration":186000,"videoId":"dklmUdpiEeW0kxK19kQG-w","assets":[]}},"slug":"stochastic-gradient-learning-one-data-point-at-a-time","timeCommitment":186000},{"id":"c7T3x","name":"Comparing gradient to stochastic gradient","content":{"typeName":"lecture","definition":{"duration":223000,"videoId":"hUm7rtpiEeWXuxJgUJEB-Q","assets":[]}},"slug":"comparing-gradient-to-stochastic-gradient","timeCommitment":223000}],"slug":"scaling-ml-with-stochastic-gradient","timeCommitment":639000},{"id":"KAzXo","name":"Understanding why stochastic gradient works","elements":[{"id":"M45d6","name":"Why would stochastic gradient ever work?","content":{"typeName":"lecture","definition":{"duration":252000,"videoId":"tjjkmNpiEeWNPApfY59gSw","assets":[]}},"slug":"why-would-stochastic-gradient-ever-work","timeCommitment":252000},{"id":"m36Rf","name":"Convergence paths","content":{"typeName":"lecture","definition":{"duration":135000,"videoId":"xVOe7dpiEeWNPApfY59gSw","assets":[]}},"slug":"convergence-paths","timeCommitment":135000}],"slug":"understanding-why-stochastic-gradient-works","timeCommitment":387000},{"id":"cXoBD","name":"Stochastic gradient: Practical tricks","elements":[{"id":"GnfLr","name":"Shuffle data before running stochastic gradient","content":{"typeName":"lecture","definition":{"duration":132000,"videoId":"0FyiYNpiEeWMwg7LWGlGpQ","assets":[]}},"slug":"shuffle-data-before-running-stochastic-gradient","timeCommitment":132000},{"id":"EUs5p","name":"Choosing step size","content":{"typeName":"lecture","definition":{"duration":220000,"videoId":"2Hn3FNpiEeWMwg7LWGlGpQ","assets":[]}},"slug":"choosing-step-size","timeCommitment":220000},{"id":"iDOIY","name":"Don't trust last coefficients","content":{"typeName":"lecture","definition":{"duration":100000,"videoId":"IR-Hc9pjEeWXuxJgUJEB-Q","assets":[]}},"slug":"don-t-trust-last-coefficients","timeCommitment":100000},{"id":"H1cmY","name":"(OPTIONAL) Learning from batches of data","content":{"typeName":"lecture","definition":{"duration":227000,"videoId":"Z9nqENpjEeWgOApRIOF5pQ","assets":[]}},"slug":"optional-learning-from-batches-of-data","timeCommitment":227000},{"id":"MWjop","name":"(OPTIONAL) Measuring convergence","content":{"typeName":"lecture","definition":{"duration":240000,"videoId":"hXNGZNpjEeWUtQpvX4iAkw","assets":[]}},"slug":"optional-measuring-convergence","timeCommitment":240000},{"id":"J2Ya1","name":"(OPTIONAL) Adding regularization","content":{"typeName":"lecture","definition":{"duration":180000,"videoId":"ryDVLtpjEeWNPApfY59gSw","assets":[]}},"slug":"optional-adding-regularization","timeCommitment":180000}],"slug":"stochastic-gradient-practical-tricks","timeCommitment":1099000},{"id":"GKNXD","name":"Online learning: Fitting models from streaming data","elements":[{"id":"5c7hE","name":"The online learning task","content":{"typeName":"lecture","definition":{"duration":204000,"videoId":"wRWxmdpjEeWUtQpvX4iAkw","assets":[]}},"slug":"the-online-learning-task","timeCommitment":204000},{"id":"g4r4n","name":"Using stochastic gradient for online learning","content":{"typeName":"lecture","definition":{"duration":231000,"videoId":"1PITB9pjEeW0kxK19kQG-w","assets":[]}},"slug":"using-stochastic-gradient-for-online-learning","timeCommitment":231000}],"slug":"online-learning-fitting-models-from-streaming-data","timeCommitment":435000},{"id":"z5ldK","name":"Summarizing scaling to huge datasets & online learning","elements":[{"id":"hrlZB","name":"Scaling to huge datasets through parallelization & module recap","content":{"typeName":"lecture","definition":{"duration":92000,"videoId":"6drb-tpjEeWgOApRIOF5pQ","assets":[]}},"slug":"scaling-to-huge-datasets-through-parallelization-module-recap","timeCommitment":92000},{"id":"hJdXC","name":"Scaling to Huge Datasets & Online Learning","content":{"typeName":"exam","definition":{"questionCount":10,"assessmentId":"4Kayp-BOEeWXrxKegFtl-w@3","gradingWeight":4,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"scaling-to-huge-datasets-online-learning","timeCommitment":1200000}],"slug":"summarizing-scaling-to-huge-datasets-online-learning","timeCommitment":1292000},{"id":"jrDlN","name":"Programming Assignment","elements":[{"id":"ZQ1aY","name":"Training Logistic Regression via Stochastic Gradient Ascent","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"hczHJdpaEeWUtQpvX4iAkw@24"}},"slug":"training-logistic-regression-via-stochastic-gradient-ascent","timeCommitment":600000},{"id":"UWEJn","name":"Training Logistic Regression via Stochastic Gradient Ascent","content":{"typeName":"exam","definition":{"questionCount":12,"assessmentId":"mf3YbNpaEeWJwRKcpT8ChQ@12","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":12,"learningObjectives":[]}},"slug":"training-logistic-regression-via-stochastic-gradient-ascent","timeCommitment":1440000}],"slug":"programming-assignment","timeCommitment":2040000}],"slug":"scaling-to-huge-datasets-online-learning","timeCommitment":6956000}]}}