{"slug":"ml-regression","name":"Machine Learning: Regression","description":"Case Study - Predicting Housing Prices\n\nIn our first case study, predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  This is just one of the many places where regression can be applied.  Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.\n\nIn this course, you will explore regularized linear regression models for the task of prediction and feature selection.  You will be able to handle very large sets of features and select between models of various complexity.  You will also analyze the impact of aspects of your data -- such as outliers -- on your selected models and predictions.  To fit these models, you will implement optimization algorithms that scale to large datasets.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Describe the input and output of a regression model.\n   -Compare and contrast bias and variance when modeling data.\n   -Estimate model parameters using optimization algorithms.\n   -Tune parameters with cross validation.\n   -Analyze the performance of the model.\n   -Describe the notion of sparsity and how LASSO leads to sparse solutions.\n   -Deploy methods to select between models.\n   -Exploit the model to form predictions. \n   -Build a regression model to predict prices using a housing dataset.\n   -Implement these techniques in Python.","promoPhoto":"https://coursera-course-photos.s3.amazonaws.com/ce/a819904d0011e5831033199a566ffe/gears-818461_1280.jpg","primaryLanguageCodes":["en"],"subtitleLanguageCodes":[],"estimatedWorkload":"6 weeks of study, 5-8 hours/week","instructorIds":[14032411,14033965],"partnerIds":[15],"categoryIds":[1],"overridePartnerLogos":{},"plannedLaunchDate":"November 30, 2015","domainTypes":[{"subdomainId":"machine-learning","domainId":"data-science"},{"subdomainId":"probability-and-statistics","domainId":"data-science"}],"faqs":[],"id":"mxdq5kIJEeWC4g7VhG4bTQ","isReal":true,"launchedAt":1448910475476,"isVerificationEnabled":true,"enrollableSiteUserRoles":[],"verificationEnabledAt":1440699895561,"isSubtitleTranslationEnabled":true,"previewUserIds":[],"isRestrictedMembership":false,"premiumExperienceVariant":"PremiumGrading","preEnrollmentEnabledAt":1441043948076,"sessionsEnabledAt":1448905558370,"s3Prefix":"mxdq5kIJEeWC4g7VhG4bTQ","courseMaterial":{"elements":[{"id":"sGAms","name":"Welcome","description":"Regression is one of the most important and broadly used machine learning and statistics tools out there.  It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response.  Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.","elements":[{"id":"7RQqB","name":"What is this course about?","elements":[{"id":"ve7zQ","name":"Important Update regarding the Machine Learning specialization","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"5GINXOc2EeamERKUlLvYTg@3"}},"slug":"important-update-regarding-the-machine-learning-specialization","timeCommitment":600000},{"id":"HRsZH","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"_4P-UJYYEeWOagrj7c6ypQ@4"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"wl0CW","name":"Welcome!","content":{"typeName":"lecture","definition":{"duration":111000,"videoId":"HjwqvZVQEeW8oA5fR3afVQ","inVideoAssessmentId":"FOjREZVcEeWX6Q454wAURw@3","assets":[]}},"slug":"welcome","timeCommitment":111000},{"id":"0JhTY","name":"What is the course about?","content":{"typeName":"lecture","definition":{"duration":219000,"videoId":"SJFD-pVQEeWgtAp21v7HWQ","assets":[]}},"slug":"what-is-the-course-about","timeCommitment":219000},{"id":"Jbiy3","name":"Outlining the first half of the course","content":{"typeName":"lecture","definition":{"duration":301000,"videoId":"lg2Jr5VQEeWMOA7ilSuFrQ","assets":[]}},"slug":"outlining-the-first-half-of-the-course","timeCommitment":301000},{"id":"Ukvoj","name":"Outlining the second half of the course","content":{"typeName":"lecture","definition":{"duration":333000,"videoId":"Gl77a5VREeWBjQq84UDiQw","assets":[]}},"slug":"outlining-the-second-half-of-the-course","timeCommitment":333000},{"id":"zkHg7","name":"Assumed background","content":{"typeName":"lecture","definition":{"duration":243000,"videoId":"92gGW5VQEeW9TRIacgNCgQ","assets":[]}},"slug":"assumed-background","timeCommitment":243000},{"id":"K0iFw","name":"Reading: Software tools you'll need","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"HsYM8JYaEeWMOA7ilSuFrQ@9"}},"slug":"reading-software-tools-you-ll-need","timeCommitment":600000}],"slug":"what-is-this-course-about","timeCommitment":3007000}],"slug":"welcome","timeCommitment":3007000},{"id":"9crXk","name":"Simple Linear Regression","description":"Our course starts from the most basic regression model: Just fitting a line to data.  This simple model for forming predictions from a single, univariate feature of the data is appropriately called \"simple linear regression\".<p> In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent.  Based on this fitted function, you will interpret the estimated model parameters and form predictions.  You will also analyze the sensitivity of your fit to outlying observations.<p> You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house.","elements":[{"id":"zoNCC","name":"Regression fundamentals","elements":[{"id":"jBqhk","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"IS8SnpYZEeWGvQ63NJJQFw@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"G12Qp","name":"A case study in predicting house prices","content":{"typeName":"lecture","definition":{"duration":62000,"videoId":"BWFAt5FXEeWdZQ6p54RTyQ","assets":[]}},"slug":"a-case-study-in-predicting-house-prices","timeCommitment":62000},{"id":"M6cMZ","name":"Regression fundamentals: data & model","content":{"typeName":"lecture","definition":{"duration":513000,"videoId":"EE8avpFXEeWXtQobFfGinQ","assets":[]}},"slug":"regression-fundamentals-data-model","timeCommitment":513000},{"id":"Gs2Il","name":"Regression fundamentals: the task","content":{"typeName":"lecture","definition":{"duration":173000,"videoId":"GDCe7pFXEeWkEhKqE1LYiw","assets":[]}},"slug":"regression-fundamentals-the-task","timeCommitment":173000},{"id":"fKsPh","name":"Regression ML block diagram","content":{"typeName":"lecture","definition":{"duration":280000,"videoId":"IHHPsZFXEeWLoQ7drs43Qw","assets":[]}},"slug":"regression-ml-block-diagram","timeCommitment":280000}],"slug":"regression-fundamentals","timeCommitment":1628000},{"id":"FkeBI","name":"The simple linear regression model, its use, and interpretation","elements":[{"id":"N8p7w","name":"The simple linear regression model","content":{"typeName":"lecture","definition":{"duration":122000,"videoId":"KHbhbpFXEeW9sw765qdK4Q","assets":[]}},"slug":"the-simple-linear-regression-model","timeCommitment":122000},{"id":"WYPGc","name":"The cost of using a given line","content":{"typeName":"lecture","definition":{"duration":404000,"videoId":"NR-yrZFXEeWY8g5Od45WKQ","assets":[]}},"slug":"the-cost-of-using-a-given-line","timeCommitment":404000},{"id":"RjYbf","name":"Using the fitted line","content":{"typeName":"lecture","definition":{"duration":391000,"videoId":"PiSoCZFXEeWu3BKy-OQ6IQ","assets":[]}},"slug":"using-the-fitted-line","timeCommitment":391000},{"id":"x8ohF","name":"Interpreting the fitted line","content":{"typeName":"lecture","definition":{"duration":384000,"videoId":"Rdl3XpFXEeWiUwofNmHWVw","assets":[]}},"slug":"interpreting-the-fitted-line","timeCommitment":384000}],"slug":"the-simple-linear-regression-model-its-use-and-interpretation","timeCommitment":1301000},{"id":"2LpZF","name":"An aside on optimization: one dimensional objectives","elements":[{"id":"a1QCT","name":"Defining our least squares optimization objective","content":{"typeName":"lecture","definition":{"duration":212000,"videoId":"xbBdAJFXEeWTOw5c75GXtQ","assets":[]}},"slug":"defining-our-least-squares-optimization-objective","timeCommitment":212000},{"id":"RUtxG","name":"Finding maxima or minima analytically","content":{"typeName":"lecture","definition":{"duration":424000,"videoId":"wtn_c5lTEeW8tA6l0lcBAQ","assets":[]}},"slug":"finding-maxima-or-minima-analytically","timeCommitment":424000},{"id":"wN0CA","name":"Maximizing a 1d function: a worked example","content":{"typeName":"lecture","definition":{"duration":178000,"videoId":"4XF3iJFXEeWgjgqcH7F57Q","assets":[]}},"slug":"maximizing-a-1d-function-a-worked-example","timeCommitment":178000},{"id":"O4j1e","name":"Finding the max via hill climbing","content":{"typeName":"lecture","definition":{"duration":404000,"videoId":"7XIgyZFXEeWgjgqcH7F57Q","assets":[]}},"slug":"finding-the-max-via-hill-climbing","timeCommitment":404000},{"id":"zVcGn","name":"Finding the min via hill descent","content":{"typeName":"lecture","definition":{"duration":215000,"videoId":"93Et2pFXEeWHQg4qmLDtHQ","assets":[]}},"slug":"finding-the-min-via-hill-descent","timeCommitment":215000},{"id":"3UvFZ","name":"Choosing stepsize and convergence criteria","content":{"typeName":"lecture","definition":{"duration":375000,"videoId":"_45brJFXEeWkEhKqE1LYiw","assets":[]}},"slug":"choosing-stepsize-and-convergence-criteria","timeCommitment":375000}],"slug":"an-aside-on-optimization-one-dimensional-objectives","timeCommitment":1808000},{"id":"52TFR","name":"An aside on optimization: multidimensional objectives","elements":[{"id":"ZwU5b","name":"Gradients: derivatives in multiple dimensions","content":{"typeName":"lecture","definition":{"duration":337000,"videoId":"CxGYMZFYEeW87Q581wtXCw","assets":[]}},"slug":"gradients-derivatives-in-multiple-dimensions","timeCommitment":337000},{"id":"6PJ3h","name":"Gradient descent: multidimensional hill descent","content":{"typeName":"lecture","definition":{"duration":360000,"videoId":"Eg9MB5FYEeWLoQ7drs43Qw","assets":[]}},"slug":"gradient-descent-multidimensional-hill-descent","timeCommitment":360000}],"slug":"an-aside-on-optimization-multidimensional-objectives","timeCommitment":697000},{"id":"2aTBf","name":"Finding the least squares line","elements":[{"id":"PRcIZ","name":"Computing the gradient of RSS","content":{"typeName":"lecture","definition":{"duration":438000,"videoId":"HPhrO5FYEeWB6QoKCTFWlw","assets":[]}},"slug":"computing-the-gradient-of-rss","timeCommitment":438000},{"id":"G9oBu","name":"Approach 1: closed-form solution","content":{"typeName":"lecture","definition":{"duration":333000,"videoId":"Iy6I_JFYEeWlDRL2KYOBdw","assets":[]}},"slug":"approach-1-closed-form-solution","timeCommitment":333000},{"id":"I0crX","name":"Optional reading: worked-out example for closed-form solution","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"h-dNDboKEeWZ6g4yeMPOtQ@4"}},"slug":"optional-reading-worked-out-example-for-closed-form-solution","timeCommitment":600000},{"id":"Ifx9C","name":"Approach 2: gradient descent","content":{"typeName":"lecture","definition":{"duration":432000,"videoId":"KalSMZFYEeW9sw765qdK4Q","assets":[]}},"slug":"approach-2-gradient-descent","timeCommitment":432000},{"id":"TFq2w","name":"Optional reading: worked-out example for gradient descent","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"89QgTLoLEeWkERJZnf4FjQ@2"}},"slug":"optional-reading-worked-out-example-for-gradient-descent","timeCommitment":600000},{"id":"5oBEV","name":"Comparing the approaches","content":{"typeName":"lecture","definition":{"duration":98000,"videoId":"MIQUhJFYEeW0RBLKMWLA5w","assets":[]}},"slug":"comparing-the-approaches","timeCommitment":98000}],"slug":"finding-the-least-squares-line","timeCommitment":2501000},{"id":"vkdlc","name":"Discussion and summary of simple linear regression","elements":[{"id":"qh40U","name":"Download notebooks to follow along","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"2nSoGJcPEeWMOA7ilSuFrQ@7"}},"slug":"download-notebooks-to-follow-along","timeCommitment":600000},{"id":"j8uCu","name":"Influence of high leverage points: exploring the data","content":{"typeName":"lecture","definition":{"duration":287000,"videoId":"T9BRxpVREeWBjQq84UDiQw","assets":[]}},"slug":"influence-of-high-leverage-points-exploring-the-data","timeCommitment":287000},{"id":"5o6uM","name":"Influence of high leverage points: removing Center City","content":{"typeName":"lecture","definition":{"duration":427000,"videoId":"txNqspWqEeWBjQq84UDiQw","assets":[]}},"slug":"influence-of-high-leverage-points-removing-center-city","timeCommitment":427000},{"id":"WD54j","name":"Influence of high leverage points: removing high-end towns","content":{"typeName":"lecture","definition":{"duration":185000,"videoId":"ZPWGgJVREeW29A559YWgsw","assets":[]}},"slug":"influence-of-high-leverage-points-removing-high-end-towns","timeCommitment":185000},{"id":"JdRnN","name":"Asymmetric cost functions","content":{"typeName":"lecture","definition":{"duration":213000,"videoId":"bfouVJFYEeWreBJmIEbY_Q","assets":[]}},"slug":"asymmetric-cost-functions","timeCommitment":213000},{"id":"UdYun","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":76000,"videoId":"f1giUJFYEeWnJxI98CrIFw","assets":[]}},"slug":"a-brief-recap","timeCommitment":76000},{"id":"Y86x5","name":"Simple Linear Regression","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"SkUNiZa-EeW1qAqjwWCw7Q@13","gradingWeight":3,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":7,"learningObjectives":[]}},"slug":"simple-linear-regression","timeCommitment":840000}],"slug":"discussion-and-summary-of-simple-linear-regression","timeCommitment":2628000},{"id":"cy67L","name":"Programming assignment","elements":[{"id":"z0Uef","name":"Reading: Fitting a simple linear regression model on housing data","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"tghYso-5EeWdpxKCd-O9eQ@15"}},"slug":"reading-fitting-a-simple-linear-regression-model-on-housing-data","timeCommitment":600000},{"id":"B6CdC","name":"Fitting a simple linear regression model on housing data","content":{"typeName":"exam","definition":{"questionCount":4,"assessmentId":"vfIaeI-5EeW6IhKawrdnuQ@9","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"fitting-a-simple-linear-regression-model-on-housing-data","timeCommitment":480000}],"slug":"programming-assignment","timeCommitment":1080000}],"slug":"simple-linear-regression","timeCommitment":11643000},{"id":"MFwVC","name":"Multiple Regression","description":"The next step in moving beyond simple linear regression is to consider \"multiple regression\" where multiple features of the data are used to form predictions.  <p> More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., 'square feet') and the observed response (like 'house sales price').  This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value.  You will also learn how to incorporate multiple input variables (e.g., 'square feet', '# bedrooms', '# bathrooms').  You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple \"features\".   Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions. <p>Here, you will also implement a gradient descent algorithm for fitting a multiple regression model.","elements":[{"id":"YfNFe","name":"Multiple features of one input","elements":[{"id":"cJ6iH","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"OvA-JZYZEeWj-hLVp2Pm8w@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"DUMg4","name":"Multiple regression intro","content":{"typeName":"lecture","definition":{"duration":30000,"videoId":"dwLQZpFcEeWLoQ7drs43Qw","assets":[]}},"slug":"multiple-regression-intro","timeCommitment":30000},{"id":"hMhl1","name":"Polynomial regression","content":{"typeName":"lecture","definition":{"duration":225000,"videoId":"dP-3_ZFdEeWHQg4qmLDtHQ","assets":[]}},"slug":"polynomial-regression","timeCommitment":225000},{"id":"hFRqn","name":"Modeling seasonality","content":{"typeName":"lecture","definition":{"duration":492000,"videoId":"oK01FpFdEeWbnw7LA_pspQ","assets":[]}},"slug":"modeling-seasonality","timeCommitment":492000},{"id":"SHy67","name":"Where we see seasonality","content":{"typeName":"lecture","definition":{"duration":201000,"videoId":"hLGDGZFdEeWtDg7-HPvApQ","assets":[]}},"slug":"where-we-see-seasonality","timeCommitment":201000},{"id":"tw28v","name":"Regression with general features of 1 input","content":{"typeName":"lecture","definition":{"duration":153000,"videoId":"g5H-bJFcEeWgjgqcH7F57Q","assets":[]}},"slug":"regression-with-general-features-of-1-input","timeCommitment":153000}],"slug":"multiple-features-of-one-input","timeCommitment":1701000},{"id":"NAyiP","name":"Incorporating multiple inputs","elements":[{"id":"hO8AO","name":"Motivating the use of multiple inputs","content":{"typeName":"lecture","definition":{"duration":270000,"videoId":"jldCbpFcEeWY8g5Od45WKQ","assets":[]}},"slug":"motivating-the-use-of-multiple-inputs","timeCommitment":270000},{"id":"QGZbl","name":"Defining notation","content":{"typeName":"lecture","definition":{"duration":199000,"videoId":"nAb6ypFcEeWXtQobFfGinQ","assets":[]}},"slug":"defining-notation","timeCommitment":199000},{"id":"5fzQC","name":"Regression with features of multiple inputs","content":{"typeName":"lecture","definition":{"duration":233000,"videoId":"p8Gau5FcEeWOewoNopqiww","assets":[]}},"slug":"regression-with-features-of-multiple-inputs","timeCommitment":233000},{"id":"1mwF2","name":"Interpreting the multiple regression fit","content":{"typeName":"lecture","definition":{"duration":430000,"videoId":"tp14fZFcEeWtDg7-HPvApQ","assets":[]}},"slug":"interpreting-the-multiple-regression-fit","timeCommitment":430000}],"slug":"incorporating-multiple-inputs","timeCommitment":1132000},{"id":"92NQc","name":"Setting the stage for computing the least squares fit","elements":[{"id":"vHEr6","name":"Optional reading: review of matrix algebra","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"iFBCmJr-EeW2BBISrFFbMw@3"}},"slug":"optional-reading-review-of-matrix-algebra","timeCommitment":600000},{"id":"YWLRQ","name":"Rewriting the single observation model in vector notation","content":{"typeName":"lecture","definition":{"duration":405000,"videoId":"3Is__ZGmEeWX5gqt1BX-yw","assets":[]}},"slug":"rewriting-the-single-observation-model-in-vector-notation","timeCommitment":405000},{"id":"8OiEb","name":"Rewriting the model for all observations in matrix notation","content":{"typeName":"lecture","definition":{"duration":275000,"videoId":"zHS_opFcEeWY8g5Od45WKQ","assets":[]}},"slug":"rewriting-the-model-for-all-observations-in-matrix-notation","timeCommitment":275000},{"id":"82RKt","name":"Computing the cost of a D-dimensional curve","content":{"typeName":"lecture","definition":{"duration":583000,"videoId":"77dkGpFcEeWB6QoKCTFWlw","assets":[]}},"slug":"computing-the-cost-of-a-d-dimensional-curve","timeCommitment":583000}],"slug":"setting-the-stage-for-computing-the-least-squares-fit","timeCommitment":1863000},{"id":"jWUcS","name":"Computing the least squares D-dimensional curve","elements":[{"id":"IYnxj","name":"Computing the gradient of RSS","content":{"typeName":"lecture","definition":{"duration":194000,"videoId":"EXJ9rZFdEeWX5gqt1BX-yw","assets":[]}},"slug":"computing-the-gradient-of-rss","timeCommitment":194000},{"id":"zxirl","name":"Approach 1: closed-form solution","content":{"typeName":"lecture","definition":{"duration":238000,"videoId":"HnLjsJFdEeW8NBIW8xjoFQ","assets":[]}},"slug":"approach-1-closed-form-solution","timeCommitment":238000},{"id":"jOVX8","name":"Discussing the closed-form solution","content":{"typeName":"lecture","definition":{"duration":286000,"videoId":"KbrEQpFdEeWu3BKy-OQ6IQ","assets":[]}},"slug":"discussing-the-closed-form-solution","timeCommitment":286000},{"id":"0v3EJ","name":"Approach 2: gradient descent","content":{"typeName":"lecture","definition":{"duration":123000,"videoId":"OWpsjJFdEeWiUwofNmHWVw","assets":[]}},"slug":"approach-2-gradient-descent","timeCommitment":123000},{"id":"EtPvl","name":"Feature-by-feature update","content":{"typeName":"lecture","definition":{"duration":540000,"videoId":"RDXxVZFdEeWSsQqpBbpDcw","assets":[]}},"slug":"feature-by-feature-update","timeCommitment":540000},{"id":"SWpjC","name":"Algorithmic summary of gradient descent approach","content":{"typeName":"lecture","definition":{"duration":243000,"videoId":"UO7uNJFdEeW3pwppHIbtlQ","assets":[]}},"slug":"algorithmic-summary-of-gradient-descent-approach","timeCommitment":243000}],"slug":"computing-the-least-squares-d-dimensional-curve","timeCommitment":1624000},{"id":"wiEW3","name":"Summarizing multiple regression","elements":[{"id":"dfzIk","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":71000,"videoId":"fMBHOpFdEeWkEhKqE1LYiw","assets":[]}},"slug":"a-brief-recap","timeCommitment":71000},{"id":"qlNDi","name":"Multiple Regression","content":{"typeName":"exam","definition":{"questionCount":9,"assessmentId":"-DsP3ZbCEeWgtAp21v7HWQ@17","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":9,"learningObjectives":[]}},"slug":"multiple-regression","timeCommitment":1080000}],"slug":"summarizing-multiple-regression","timeCommitment":1151000},{"id":"FM0be","name":"Programming assignment 1","elements":[{"id":"7xN9c","name":"Reading: Exploring different multiple regression models for house price prediction","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"uWpIMY-5EeWcXQ6fTaPbmQ@15"}},"slug":"reading-exploring-different-multiple-regression-models-for-house-price","timeCommitment":600000},{"id":"KXEJ3","name":"Exploring different multiple regression models for house price prediction","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"vwYiZo-5EeWcPwoWNBf8JQ@4","gradingWeight":7,"passingFraction":1,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"exploring-different-multiple-regression-models-for-house-price-prediction","timeCommitment":960000}],"slug":"programming-assignment-1","timeCommitment":1560000},{"id":"lpfkB","name":"Programming assignment 2","elements":[{"id":"ArRj2","name":"Numpy tutorial","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"w0SwdpZlEeWo-RIhkWTWcw@7"}},"slug":"numpy-tutorial","timeCommitment":600000},{"id":"tcfe4","name":"Reading: Implementing gradient descent for multiple regression","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"uviwMI-5EeWVtg7O8Wlmbw@20"}},"slug":"reading-implementing-gradient-descent-for-multiple-regression","timeCommitment":600000},{"id":"gCGSX","name":"Implementing gradient descent for multiple regression","content":{"typeName":"exam","definition":{"questionCount":5,"assessmentId":"wBx0do-5EeW0QhLL_rCzXw@7","gradingWeight":7,"passingFraction":1,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"implementing-gradient-descent-for-multiple-regression","timeCommitment":600000}],"slug":"programming-assignment-2","timeCommitment":1800000}],"slug":"multiple-regression","timeCommitment":10831000},{"id":"IjyId","name":"Assessing Performance","description":"Having learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data.  You are also ready to select amongst possible models to choose the best performing.  <p> This module is all about these important topics of model selection and assessment.  You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the \"loss\" of your predictions, and use this to define training, test, and generalization error.  For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance.  This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning.  Finally, you will devise a method to first select amongst models and then assess the performance of the selected model. <p>The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course.","elements":[{"id":"3nn0T","name":"Defining how we assess performance","elements":[{"id":"tLzRX","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"UbWOyJYZEeWkIQ4LGQjrgQ@3"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"EC0kO","name":"Assessing performance intro","content":{"typeName":"lecture","definition":{"duration":32000,"videoId":"m9qpUpFeEeWOewoNopqiww","assets":[]}},"slug":"assessing-performance-intro","timeCommitment":32000},{"id":"cGUQ3","name":"What do we mean by \"loss\"?","content":{"typeName":"lecture","definition":{"duration":256000,"videoId":"0pnUKZFhEeWreBJmIEbY_Q","assets":[]}},"slug":"what-do-we-mean-by-loss","timeCommitment":256000}],"slug":"defining-how-we-assess-performance","timeCommitment":888000},{"id":"QlHA2","name":"3 measures of loss and their trends with model complexity","elements":[{"id":"VN4Qo","name":"Training error: assessing loss on the training set","content":{"typeName":"lecture","definition":{"duration":455000,"videoId":"OtwMAJFiEeW8NBIW8xjoFQ","assets":[]}},"slug":"training-error-assessing-loss-on-the-training-set","timeCommitment":455000},{"id":"CDx5h","name":"Generalization error: what we really want","content":{"typeName":"lecture","definition":{"duration":499000,"videoId":"X23r7ZFiEeWdZQ6p54RTyQ","assets":[]}},"slug":"generalization-error-what-we-really-want","timeCommitment":499000},{"id":"pq0SM","name":"Test error: what we can actually compute","content":{"typeName":"lecture","definition":{"duration":293000,"videoId":"1b2pTZFhEeWkEhKqE1LYiw","assets":[]}},"slug":"test-error-what-we-can-actually-compute","timeCommitment":293000},{"id":"u8c2x","name":"Defining overfitting","content":{"typeName":"lecture","definition":{"duration":153000,"videoId":"khvuOpFgEeW9sw765qdK4Q","assets":[]}},"slug":"defining-overfitting","timeCommitment":153000},{"id":"qn2vj","name":"Training/test split","content":{"typeName":"lecture","definition":{"duration":111000,"videoId":"kM4RkZFgEeWnJxI98CrIFw","assets":[]}},"slug":"training-test-split","timeCommitment":111000}],"slug":"3-measures-of-loss-and-their-trends-with-model-complexity","timeCommitment":1511000},{"id":"hri0a","name":"3 sources of error and the bias-variance tradeoff","elements":[{"id":"qlMrZ","name":"Irreducible error and bias","content":{"typeName":"lecture","definition":{"duration":386000,"videoId":"-QM0hZFhEeW8NBIW8xjoFQ","assets":[]}},"slug":"irreducible-error-and-bias","timeCommitment":386000},{"id":"ZvP40","name":"Variance and the bias-variance tradeoff","content":{"typeName":"lecture","definition":{"duration":412000,"videoId":"AXt6dZFiEeWETRLiQBhkPQ","assets":[]}},"slug":"variance-and-the-bias-variance-tradeoff","timeCommitment":412000},{"id":"lYBeX","name":"Error vs. amount of data","content":{"typeName":"lecture","definition":{"duration":387000,"videoId":"CeSlr5FiEeW9sw765qdK4Q","assets":[]}},"slug":"error-vs-amount-of-data","timeCommitment":387000}],"slug":"3-sources-of-error-and-the-bias-variance-tradeoff","timeCommitment":1185000},{"id":"6mkBL","name":"OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error","elements":[{"id":"PB7vp","name":"Formally defining the 3 sources of error","content":{"typeName":"lecture","definition":{"duration":854000,"videoId":"FOFL8ZFiEeWbnw7LA_pspQ","assets":[]}},"slug":"formally-defining-the-3-sources-of-error","timeCommitment":854000},{"id":"QiT0N","name":"Formally deriving why 3 sources of error","content":{"typeName":"lecture","definition":{"duration":1218000,"videoId":"HIoNKZFiEeWnJxI98CrIFw","assets":[]}},"slug":"formally-deriving-why-3-sources-of-error","timeCommitment":1218000}],"slug":"optional-advanced-material-formally-defining-and-deriving-the-3-sources-of-error","timeCommitment":2072000},{"id":"iEzPE","name":"Putting the pieces together","elements":[{"id":"HNJ0c","name":"Training/validation/test split for model selection, fitting, and assessment","content":{"typeName":"lecture","definition":{"duration":458000,"videoId":"gnHsJZFgEeWbnw7LA_pspQ","assets":[]}},"slug":"training-validation-test-split-for-model-selection-fitting-and-assessment","timeCommitment":458000},{"id":"FT2HG","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":85000,"videoId":"1KLygZFeEeWHQg4qmLDtHQ","assets":[]}},"slug":"a-brief-recap","timeCommitment":85000},{"id":"vveUj","name":"Assessing Performance","content":{"typeName":"exam","definition":{"questionCount":13,"assessmentId":"0Ns6HZbFEeWLyxIMYNNKjQ@5","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"assessing-performance","timeCommitment":1560000}],"slug":"putting-the-pieces-together","timeCommitment":2103000},{"id":"J3yWU","name":"Programming assignment","elements":[{"id":"MhFOa","name":"Reading: Exploring the bias-variance tradeoff","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"vFd87Y-5EeWiDw7-qbyzDQ@14"}},"slug":"reading-exploring-the-bias-variance-tradeoff","timeCommitment":600000},{"id":"wvAE9","name":"Exploring the bias-variance tradeoff","content":{"typeName":"exam","definition":{"questionCount":4,"assessmentId":"wWijio-5EeWMlArNEd9jRQ@4","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"exploring-the-bias-variance-tradeoff","timeCommitment":480000}],"slug":"programming-assignment","timeCommitment":1080000}],"slug":"assessing-performance","timeCommitment":8839000},{"id":"Uv1l0","name":"Ridge Regression","description":"You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data.   In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue.  This method is called \"ridge regression\".  You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions.  To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective.  You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression.  To select the strength of the bias away from overfitting, you will explore a general-purpose method called \"cross validation\". <p>You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant.","elements":[{"id":"wzbDS","name":"Characteristics of overfit models","elements":[{"id":"mU7U8","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"Y1354pYZEeWLyxIMYNNKjQ@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"TIGJ5","name":"Symptoms of overfitting in polynomial regression","content":{"typeName":"lecture","definition":{"duration":127000,"videoId":"KQWYRZF9EeWgjgqcH7F57Q","assets":[]}},"slug":"symptoms-of-overfitting-in-polynomial-regression","timeCommitment":127000},{"id":"M3N2a","name":"Download the notebook and follow along","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"4xwC2ZcREeWGvQ63NJJQFw@5"}},"slug":"download-the-notebook-and-follow-along","timeCommitment":600000},{"id":"o6wzg","name":"Overfitting demo","content":{"typeName":"lecture","definition":{"duration":424000,"videoId":"wps4DZVREeWY_RLdvg01cw","assets":[]}},"slug":"overfitting-demo","timeCommitment":424000},{"id":"38mGi","name":"Overfitting for more general multiple regression models","content":{"typeName":"lecture","definition":{"duration":211000,"videoId":"PMRfYZF9EeWHQg4qmLDtHQ","assets":[]}},"slug":"overfitting-for-more-general-multiple-regression-models","timeCommitment":211000}],"slug":"characteristics-of-overfit-models","timeCommitment":1962000},{"id":"GpRUm","name":"The ridge objective","elements":[{"id":"b1fbX","name":"Balancing fit and magnitude of coefficients","content":{"typeName":"lecture","definition":{"duration":428000,"videoId":"SYCQwJF9EeWkEhKqE1LYiw","assets":[]}},"slug":"balancing-fit-and-magnitude-of-coefficients","timeCommitment":428000},{"id":"xYzTA","name":"The resulting ridge objective and its extreme solutions","content":{"typeName":"lecture","definition":{"duration":356000,"videoId":"VjdEOpF9EeWtDg7-HPvApQ","assets":[]}},"slug":"the-resulting-ridge-objective-and-its-extreme-solutions","timeCommitment":356000},{"id":"697eG","name":"How ridge regression balances bias and variance","content":{"typeName":"lecture","definition":{"duration":93000,"videoId":"bYJmV5F9EeWLoQ7drs43Qw","assets":[]}},"slug":"how-ridge-regression-balances-bias-and-variance","timeCommitment":93000},{"id":"dBWKM","name":"Download the notebook and follow along","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"ZwHhGpcSEeW29A559YWgsw@3"}},"slug":"download-the-notebook-and-follow-along","timeCommitment":600000},{"id":"Dh4Qw","name":"Ridge regression demo","content":{"typeName":"lecture","definition":{"duration":548000,"videoId":"-d95b5VREeWHjgq3COkl3w","assets":[]}},"slug":"ridge-regression-demo","timeCommitment":548000},{"id":"Ki05k","name":"The ridge coefficient path","content":{"typeName":"lecture","definition":{"duration":247000,"videoId":"gsra8JF9EeWbnw7LA_pspQ","assets":[]}},"slug":"the-ridge-coefficient-path","timeCommitment":247000}],"slug":"the-ridge-objective","timeCommitment":2272000},{"id":"xsLLe","name":"Optimizing the ridge objective","elements":[{"id":"kvaqc","name":"Computing the gradient of the ridge objective","content":{"typeName":"lecture","definition":{"duration":303000,"videoId":"oN3UNZF9EeWdZQ6p54RTyQ","assets":[]}},"slug":"computing-the-gradient-of-the-ridge-objective","timeCommitment":303000},{"id":"uYzai","name":"Approach 1: closed-form solution","content":{"typeName":"lecture","definition":{"duration":362000,"videoId":"85tRPJF9EeWB6QoKCTFWlw","assets":[]}},"slug":"approach-1-closed-form-solution","timeCommitment":362000},{"id":"SlFBr","name":"Discussing the closed-form solution","content":{"typeName":"lecture","definition":{"duration":348000,"videoId":"_hJ0n5F9EeW87Q581wtXCw","assets":[]}},"slug":"discussing-the-closed-form-solution","timeCommitment":348000},{"id":"v9vA8","name":"Approach 2: gradient descent","content":{"typeName":"lecture","definition":{"duration":575000,"videoId":"EGkhLpF-EeWLoQ7drs43Qw","assets":[]}},"slug":"approach-2-gradient-descent","timeCommitment":575000}],"slug":"optimizing-the-ridge-objective","timeCommitment":1588000},{"id":"6ZbRR","name":"Tying up the loose ends","elements":[{"id":"UEH0h","name":"Selecting tuning parameters via cross validation","content":{"typeName":"lecture","definition":{"duration":235000,"videoId":"1uQ2lpGwEeWX5gqt1BX-yw","assets":[]}},"slug":"selecting-tuning-parameters-via-cross-validation","timeCommitment":235000},{"id":"FJcUw","name":"K-fold cross validation","content":{"typeName":"lecture","definition":{"duration":334000,"videoId":"E-HrlZGxEeWETRLiQBhkPQ","assets":[]}},"slug":"k-fold-cross-validation","timeCommitment":334000},{"id":"3KZiN","name":"How to handle the intercept","content":{"typeName":"lecture","definition":{"duration":386000,"videoId":"WpndPpGxEeWETRLiQBhkPQ","assets":[]}},"slug":"how-to-handle-the-intercept","timeCommitment":386000},{"id":"ZF0AO","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":104000,"videoId":"eRShIJF-EeWiUwofNmHWVw","assets":[]}},"slug":"a-brief-recap","timeCommitment":104000},{"id":"REIRy","name":"Ridge Regression","content":{"typeName":"exam","definition":{"questionCount":9,"assessmentId":"WV9A4pabEeWG6hLTkhDuTw@7","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":9,"learningObjectives":[]}},"slug":"ridge-regression","timeCommitment":1080000}],"slug":"tying-up-the-loose-ends","timeCommitment":2139000},{"id":"mL4T4","name":"Programming Assignment 1","elements":[{"id":"64hYu","name":"Reading: Observing effects of L2 penalty in polynomial regression","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"Hv2Fl5VYEeWlLg7xqmxkfw@21"}},"slug":"reading-observing-effects-of-l2-penalty-in-polynomial-regression","timeCommitment":600000},{"id":"A0as6","name":"Observing effects of L2 penalty in polynomial regression","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"m5hzc5VbEeW8JgoAj3l_iw@16","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":7,"learningObjectives":[]}},"slug":"observing-effects-of-l2-penalty-in-polynomial-regression","timeCommitment":840000}],"slug":"programming-assignment-1","timeCommitment":1440000},{"id":"D7Z9O","name":"Programming Assignment 2","elements":[{"id":"poz6z","name":"Reading: Implementing ridge regression via gradient descent","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"e3k2MZWiEeW1qAqjwWCw7Q@12"}},"slug":"reading-implementing-ridge-regression-via-gradient-descent","timeCommitment":600000},{"id":"6JDi6","name":"Implementing ridge regression via gradient descent","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"L0JP0JWnEeWIMBL2XociMQ@4","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"implementing-ridge-regression-via-gradient-descent","timeCommitment":960000}],"slug":"programming-assignment-2","timeCommitment":1560000}],"slug":"ridge-regression","timeCommitment":10961000},{"id":"acwch","name":"Feature Selection & Lasso","description":"A fundamental machine learning task is to select amongst a set of features to include in a model.  In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions. <p> To start, you will examine methods that search over an enumeration of models including different subsets of features.  You will analyze both exhaustive search and greedy algorithms.  Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge.  This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model. <p>Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning. ","elements":[{"id":"Zvp6H","name":"Feature selection via explicit model enumeration","elements":[{"id":"126gR","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"c4JcNpYZEeW8oA5fR3afVQ@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"sBD0S","name":"The feature selection task","content":{"typeName":"lecture","definition":{"duration":225000,"videoId":"ewmIIpGIEeWHQg4qmLDtHQ","assets":[]}},"slug":"the-feature-selection-task","timeCommitment":225000},{"id":"99zV1","name":"All subsets","content":{"typeName":"lecture","definition":{"duration":375000,"videoId":"qzG3OpGIEeW9sw765qdK4Q","assets":[]}},"slug":"all-subsets","timeCommitment":375000},{"id":"weWsz","name":"Complexity of all subsets","content":{"typeName":"lecture","definition":{"duration":191000,"videoId":"BkQUHJGJEeWgjgqcH7F57Q","assets":[]}},"slug":"complexity-of-all-subsets","timeCommitment":191000},{"id":"ulupn","name":"Greedy algorithms","content":{"typeName":"lecture","definition":{"duration":449000,"videoId":"FW--5JGJEeWOewoNopqiww","assets":[]}},"slug":"greedy-algorithms","timeCommitment":449000},{"id":"RRah3","name":"Complexity of the greedy forward stepwise algorithm","content":{"typeName":"lecture","definition":{"duration":159000,"videoId":"LTKo_5GJEeWX5gqt1BX-yw","assets":[]}},"slug":"complexity-of-the-greedy-forward-stepwise-algorithm","timeCommitment":159000}],"slug":"feature-selection-via-explicit-model-enumeration","timeCommitment":1999000},{"id":"OC8oX","name":"Feature selection implicitly via regularized regression","elements":[{"id":"0FyEi","name":"Can we use regularization for feature selection?","content":{"typeName":"lecture","definition":{"duration":229000,"videoId":"TUy9mJGJEeWdZQ6p54RTyQ","assets":[]}},"slug":"can-we-use-regularization-for-feature-selection","timeCommitment":229000},{"id":"Hn95c","name":"Thresholding ridge coefficients?","content":{"typeName":"lecture","definition":{"duration":280000,"videoId":"V7i935GJEeWY8g5Od45WKQ","assets":[]}},"slug":"thresholding-ridge-coefficients","timeCommitment":280000},{"id":"mw2Ul","name":"The lasso objective and its coefficient path","content":{"typeName":"lecture","definition":{"duration":423000,"videoId":"ZCQPdpGJEeWu3BKy-OQ6IQ","assets":[]}},"slug":"the-lasso-objective-and-its-coefficient-path","timeCommitment":423000}],"slug":"feature-selection-implicitly-via-regularized-regression","timeCommitment":932000},{"id":"JvzWL","name":"Geometric intuition for sparsity of lasso solutions","elements":[{"id":"p2X9f","name":"Visualizing the ridge cost","content":{"typeName":"lecture","definition":{"duration":479000,"videoId":"3e8j_JGgEeWHQg4qmLDtHQ","assets":[]}},"slug":"visualizing-the-ridge-cost","timeCommitment":479000},{"id":"1F5pH","name":"Visualizing the ridge solution","content":{"typeName":"lecture","definition":{"duration":371000,"videoId":"rs0xH5GxEeWSsQqpBbpDcw","assets":[]}},"slug":"visualizing-the-ridge-solution","timeCommitment":371000},{"id":"VZlmt","name":"Visualizing the lasso cost and solution","content":{"typeName":"lecture","definition":{"duration":470000,"videoId":"5p2e6ZMPEeW9TRIacgNCgQ","assets":[]}},"slug":"visualizing-the-lasso-cost-and-solution","timeCommitment":470000},{"id":"JrkN1","name":"Download the notebook and follow along","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"q--ZlpcSEeW29A559YWgsw@4"}},"slug":"download-the-notebook-and-follow-along","timeCommitment":600000},{"id":"oHPlv","name":"Lasso demo","content":{"typeName":"lecture","definition":{"duration":308000,"videoId":"J17IeJWqEeWj-hLVp2Pm8w","assets":[]}},"slug":"lasso-demo","timeCommitment":308000}],"slug":"geometric-intuition-for-sparsity-of-lasso-solutions","timeCommitment":2228000},{"id":"8QicT","name":"Setting the stage for solving the lasso","elements":[{"id":"TNPal","name":"What makes the lasso objective different","content":{"typeName":"lecture","definition":{"duration":210000,"videoId":"VN7K25GgEeW87Q581wtXCw","assets":[]}},"slug":"what-makes-the-lasso-objective-different","timeCommitment":210000},{"id":"uHBq7","name":"Coordinate descent","content":{"typeName":"lecture","definition":{"duration":358000,"videoId":"a0NeHpGgEeW8NBIW8xjoFQ","assets":[]}},"slug":"coordinate-descent","timeCommitment":358000},{"id":"By53l","name":"Normalizing features","content":{"typeName":"lecture","definition":{"duration":182000,"videoId":"JF3RwpbqEeWgFQ4oanzj2Q","assets":[]}},"slug":"normalizing-features","timeCommitment":182000},{"id":"wkbZU","name":"Coordinate descent for least squares regression (normalized features)","content":{"typeName":"lecture","definition":{"duration":518000,"videoId":"tmc1PJGgEeWB6QoKCTFWlw","assets":[]}},"slug":"coordinate-descent-for-least-squares-regression-normalized-features","timeCommitment":518000}],"slug":"setting-the-stage-for-solving-the-lasso","timeCommitment":1268000},{"id":"sHqWS","name":"Optimizing the lasso objective","elements":[{"id":"SeZsT","name":"Coordinate descent for lasso (normalized features)","content":{"typeName":"lecture","definition":{"duration":308000,"videoId":"TzVnGZGhEeW0RBLKMWLA5w","assets":[]}},"slug":"coordinate-descent-for-lasso-normalized-features","timeCommitment":308000},{"id":"0ikvP","name":"Assessing convergence and other lasso solvers","content":{"typeName":"lecture","definition":{"duration":176000,"videoId":"YUDYqJGhEeWgjgqcH7F57Q","assets":[]}},"slug":"assessing-convergence-and-other-lasso-solvers","timeCommitment":176000},{"id":"AsCvQ","name":"Coordinate descent for lasso (unnormalized features)","content":{"typeName":"lecture","definition":{"duration":113000,"videoId":"eRckCJGhEeWX5gqt1BX-yw","assets":[]}},"slug":"coordinate-descent-for-lasso-unnormalized-features","timeCommitment":113000}],"slug":"optimizing-the-lasso-objective","timeCommitment":597000},{"id":"qclnx","name":"OPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent update","elements":[{"id":"6OLyn","name":"Deriving the lasso coordinate descent update","content":{"typeName":"lecture","definition":{"duration":1168000,"videoId":"f5XISJGiEeWOewoNopqiww","assets":[]}},"slug":"deriving-the-lasso-coordinate-descent-update","timeCommitment":1168000}],"slug":"optional-advanced-material-deriving-the-lasso-coordinate-descent-update","timeCommitment":1168000},{"id":"Uk7hT","name":"Tying up loose ends","elements":[{"id":"SPr8p","name":"Choosing the penalty strength and other practical issues with lasso","content":{"typeName":"lecture","definition":{"duration":348000,"videoId":"NWKu7ZGhEeWtDg7-HPvApQ","assets":[]}},"slug":"choosing-the-penalty-strength-and-other-practical-issues-with-lasso","timeCommitment":348000},{"id":"s0AYj","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":212000,"videoId":"VjRvypGgEeWETRLiQBhkPQ","assets":[]}},"slug":"a-brief-recap","timeCommitment":212000},{"id":"uifj6","name":"Feature Selection and Lasso","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"SDt0RJauEeW4CBJhzr_GTw@6","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"maxScore":7,"learningObjectives":[]}},"slug":"feature-selection-and-lasso","timeCommitment":840000}],"slug":"tying-up-loose-ends","timeCommitment":1400000},{"id":"go3S5","name":"Programming Assignment 1","elements":[{"id":"qsV5O","name":"Reading: Using LASSO to select features","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"OFsX2JX4EeW8oA5fR3afVQ@14"}},"slug":"reading-using-lasso-to-select-features","timeCommitment":600000},{"id":"qtNcq","name":"Using LASSO to select features","content":{"typeName":"exam","definition":{"questionCount":6,"assessmentId":"XVoxXZX7EeW8oA5fR3afVQ@10","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"using-lasso-to-select-features","timeCommitment":720000}],"slug":"programming-assignment-1","timeCommitment":1320000},{"id":"mmGj6","name":"Programming Assignment 2","elements":[{"id":"dtsUT","name":"Reading: Implementing LASSO using coordinate descent","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"oD22UJYQEeW4CBJhzr_GTw@11"}},"slug":"reading-implementing-lasso-using-coordinate-descent","timeCommitment":600000},{"id":"Bican","name":"Implementing LASSO using coordinate descent","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"sPcszpYQEeW4CBJhzr_GTw@3","gradingWeight":7,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"implementing-lasso-using-coordinate-descent","timeCommitment":960000}],"slug":"programming-assignment-2","timeCommitment":1560000}],"slug":"feature-selection-lasso","timeCommitment":12472000},{"id":"5i0au","name":"Nearest Neighbors & Kernel Regression","description":"Up to this point, we have focused on methods that fit parametric functions---like polynomials and hyperplanes---to the entire dataset.  In this module, we instead turn our attention to a class of \"nonparametric\" methods.  These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.  <p> We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set.  This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors.  Building on this idea, we turn to kernel regression.  Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point.  You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle.  You will also implement these techniques and observe their practical behavior.","elements":[{"id":"KZuhO","name":"Motivating local fits","elements":[{"id":"QO0qQ","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"gxFcxpYZEeW1qAqjwWCw7Q@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"dPIqj","name":"Limitations of parametric regression","content":{"typeName":"lecture","definition":{"duration":222000,"videoId":"LJy2fZGkEeWkEhKqE1LYiw","assets":[]}},"slug":"limitations-of-parametric-regression","timeCommitment":222000}],"slug":"motivating-local-fits","timeCommitment":822000},{"id":"CbZVg","name":"Nearest neighbor regression","elements":[{"id":"ddazj","name":"1-Nearest neighbor regression approach","content":{"typeName":"lecture","definition":{"duration":507000,"videoId":"NfY7H5lUEeWJBwqx73GnzQ","assets":[]}},"slug":"1-nearest-neighbor-regression-approach","timeCommitment":507000},{"id":"k4XJP","name":"Distance metrics","content":{"typeName":"lecture","definition":{"duration":257000,"videoId":"mUMOQ5GoEeWSsQqpBbpDcw","assets":[]}},"slug":"distance-metrics","timeCommitment":257000},{"id":"FU3fB","name":"1-Nearest neighbor algorithm","content":{"typeName":"lecture","definition":{"duration":208000,"videoId":"q3pyqpGoEeWkEhKqE1LYiw","assets":[]}},"slug":"1-nearest-neighbor-algorithm","timeCommitment":208000}],"slug":"nearest-neighbor-regression","timeCommitment":972000},{"id":"mxBlb","name":"k-Nearest neighbors and weighted k-nearest neighbors","elements":[{"id":"8pPF9","name":"k-Nearest neighbors regression","content":{"typeName":"lecture","definition":{"duration":454000,"videoId":"KbV-e5GwEeWnJxI98CrIFw","assets":[]}},"slug":"k-nearest-neighbors-regression","timeCommitment":454000},{"id":"XJ48x","name":"k-Nearest neighbors in practice","content":{"typeName":"lecture","definition":{"duration":220000,"videoId":"ymyoYpGoEeWlDRL2KYOBdw","assets":[]}},"slug":"k-nearest-neighbors-in-practice","timeCommitment":220000},{"id":"778Pn","name":"Weighted k-nearest neighbors","content":{"typeName":"lecture","definition":{"duration":276000,"videoId":"y-HR4ZGqEeW87Q581wtXCw","assets":[]}},"slug":"weighted-k-nearest-neighbors","timeCommitment":276000}],"slug":"k-nearest-neighbors-and-weighted-k-nearest-neighbors","timeCommitment":950000},{"id":"W9Tbd","name":"Kernel regression","elements":[{"id":"5GtJy","name":"From weighted k-NN to kernel regression","content":{"typeName":"lecture","definition":{"duration":403000,"videoId":"6BUhGpGoEeWdZQ6p54RTyQ","assets":[]}},"slug":"from-weighted-k-nn-to-kernel-regression","timeCommitment":403000},{"id":"tUFQ9","name":"Global fits of parametric models vs. local fits of kernel regression","content":{"typeName":"lecture","definition":{"duration":375000,"videoId":"8oNDwJGoEeW9sw765qdK4Q","assets":[]}},"slug":"global-fits-of-parametric-models-vs-local-fits-of-kernel-regression","timeCommitment":375000}],"slug":"kernel-regression","timeCommitment":778000},{"id":"fjxFy","name":"k-NN and kernel regression wrapup","elements":[{"id":"YoLBb","name":"Performance of NN as amount of data grows","content":{"typeName":"lecture","definition":{"duration":455000,"videoId":"_9YPyZGoEeWbnw7LA_pspQ","assets":[]}},"slug":"performance-of-nn-as-amount-of-data-grows","timeCommitment":455000},{"id":"Nvixn","name":"Issues with high-dimensions, data scarcity, and computational complexity","content":{"typeName":"lecture","definition":{"duration":191000,"videoId":"C-KfhJGpEeWOewoNopqiww","assets":[]}},"slug":"issues-with-high-dimensions-data-scarcity-and-computational-complexity","timeCommitment":191000},{"id":"zXDYa","name":"k-NN for classification","content":{"typeName":"lecture","definition":{"duration":117000,"videoId":"Fpx7lpGqEeWB6QoKCTFWlw","assets":[]}},"slug":"k-nn-for-classification","timeCommitment":117000},{"id":"fJkD2","name":"A brief recap","content":{"typeName":"lecture","definition":{"duration":88000,"videoId":"fKWL8ZGqEeWreBJmIEbY_Q","assets":[]}},"slug":"a-brief-recap","timeCommitment":88000},{"id":"unsZb","name":"Nearest Neighbors & Kernel Regression","content":{"typeName":"exam","definition":{"questionCount":7,"assessmentId":"U-6ECpa3EeW1qAqjwWCw7Q@4","gradingWeight":6,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"nearest-neighbors-kernel-regression","timeCommitment":840000}],"slug":"k-nn-and-kernel-regression-wrapup","timeCommitment":1691000},{"id":"MyyDc","name":"Programming Assignment","elements":[{"id":"dQJGs","name":"Reading: Predicting house prices using k-nearest neighbors regression","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"kdo4CpYiEeWHjgq3COkl3w@13"}},"slug":"reading-predicting-house-prices-using-k-nearest-neighbors-regression","timeCommitment":600000},{"id":"QdmRA","name":"Predicting house prices using k-nearest neighbors regression","content":{"typeName":"exam","definition":{"questionCount":8,"assessmentId":"onXGnpYiEeW1qAqjwWCw7Q@3","gradingWeight":9,"passingFraction":0.8,"lockingConfiguration":{"allowedSubmissionsPerInterval":3,"allowedSubmissionsInterval":28800000},"learningObjectives":[]}},"slug":"predicting-house-prices-using-k-nearest-neighbors-regression","timeCommitment":960000}],"slug":"programming-assignment","timeCommitment":1560000}],"slug":"nearest-neighbors-kernel-regression","timeCommitment":6773000},{"id":"CO6hd","name":"Closing Remarks","description":"In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization.  We also briefly discuss some important regression techniques we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  ","elements":[{"id":"YqRiP","name":"What we've learned","elements":[{"id":"OUMXJ","name":"Slides presented in this module","content":{"typeName":"supplement","definition":{"assetTypeName":"cml","assetId":"kcQt9ZYZEeWMOA7ilSuFrQ@2"}},"slug":"slides-presented-in-this-module","timeCommitment":600000},{"id":"yNp1r","name":"Simple and multiple regression","content":{"typeName":"lecture","definition":{"duration":278000,"videoId":"qXlcupWqEeW8JgoAj3l_iw","assets":[]}},"slug":"simple-and-multiple-regression","timeCommitment":278000},{"id":"9DOzf","name":"Assessing performance and ridge regression","content":{"typeName":"lecture","definition":{"duration":424000,"videoId":"ws9VJZVTEeWOagrj7c6ypQ","assets":[]}},"slug":"assessing-performance-and-ridge-regression","timeCommitment":424000},{"id":"rb179","name":"Feature selection, lasso, and nearest neighbor regression","content":{"typeName":"lecture","definition":{"duration":251000,"videoId":"8r5NM5VTEeWgFQ4oanzj2Q","assets":[]}},"slug":"feature-selection-lasso-and-nearest-neighbor-regression","timeCommitment":251000}],"slug":"what-we-ve-learned","timeCommitment":1553000},{"id":"Tv6Vn","name":"Summary and what's ahead in the specialization","elements":[{"id":"kP5Qu","name":"What we covered and what we didn't cover","content":{"typeName":"lecture","definition":{"duration":335000,"videoId":"MXtlA5VUEeW9TRIacgNCgQ","assets":[]}},"slug":"what-we-covered-and-what-we-didn-t-cover","timeCommitment":335000},{"id":"A6xEo","name":"Thank you!","content":{"typeName":"lecture","definition":{"duration":92000,"videoId":"Wlr_OJWqEeWG6hLTkhDuTw","assets":[]}},"slug":"thank-you","timeCommitment":92000}],"slug":"summary-and-whats-ahead-in-the-specialization","timeCommitment":427000}],"slug":"closing-remarks","timeCommitment":1980000}]}}